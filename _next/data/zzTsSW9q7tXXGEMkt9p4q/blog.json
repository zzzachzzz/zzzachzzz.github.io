{"pageProps":{"allPosts":[{"title":"How I Finally Got Secure Boot Working in Debian","date":"2025-12-26T14:18:15.800Z","slug":"how-i-finally-got-secure-boot-working-in-debian","content":"\n![sudo mokutil --sb-state SecureBoot enabled](/assets/secure-boot-enabled-mokutil.jpg)\n\nTL;DR: [How I Did It](#how-i-did-it)\n\n## Intro\n\nSecure boot is actually pretty well supported on Debian and other major distros. But complications can arise when:\n\n1. Enabling secure boot later, not during a clean install of the OS\n2. Using a non-default bootloader (default is typically GRUB)\n3. Having multiple bootloaders installed\n\nAll of the above applied to my situation, making it hard mode.\n\n## Background on My Weird Setup\n\nI was single booting Arch + systemd-boot for a while.\n\nThen I adapted this setup to dual boot Arch & Debian via multiple systemd-boot boot entries.\n\nWhen Debian was installed, GRUB got installed too, somehow, despite reusing my boot partition and specifically trying to avoid this.\n\nLater I removed the Arch installation, and switched to dual booting Debian & Windows. (through multiple UEFI boot entries, couldn't quite get it working to launch Windows from systemd-boot)\n\nI enabled secure boot for a Windows game (League of Legends Vanguard anti-cheat...).\n\nNow, the problem motivating me to get secure boot working on Debian:\n**I can't boot Debian without first disabling secure boot**\n\n## Terminology\n\n- **UEFI/NVRAM boot entries** : Boot entries in motherboard menu (firmware level shit) (`Boot0001`, `Boot0002`, etc.)\n- **Bootloader** : Software (systemd-boot, GRUB) that loads the OS kernel, acting as the bridge between firmware (UEFI/BIOS) and the OS\n- **[UEFI (Unified Extensible Firmware Interface)](https://wiki.archlinux.org/title/Unified_Extensible_Firmware_Interface)** : Modern firmware standard for motherboards, successor to legacy BIOS\n- **EFI (Extensible Firmware Interface)** : `.efi` files are UEFI executables, typically bootloaders and shims, placed on the boot partition\n- **[ESP (EFI System Partition)](https://wiki.archlinux.org/title/EFI_system_partition)** : OS-independent boot partition for UEFI (typically mounted at `/boot`)\n- **Shim** : An alternative method of managing accepted Secure Boot keys without touching the UEFI firmware settings. In its simplest configuration, shim will just chainload any EFI executable.\n- **Trixie** : Debian 13 Trixie (stable release at time of writing)\n- **Bookworm** : Debian 12 Bookworm (oldstable release at time of writing)\n\n## Debian Docs and Shim Explained\n\nI was hopeful when I found this excerpt from the Debian Wiki:\n\n> Starting with Trixie, users of Debian (and of the Debian Installer, since Trixie RC3) can optionally choose to use systemd-boot with shim. The integration point is the systemd-boot package and its postinst/prerm scripts. If GRUB packages are not installed, and both the shim-signed and systemd-boot-efi-[amd64|arm64]-signed packages are installed, then the maintainer scripts will run logic to install the appropriate EFI binaries to the ESP, and to add an EFI boot entry (named Debian) pointing to shim and making it the default entry. If either or both packages are removed, this logic runs in reverse and completely removes the binaries from the ESP, and deletes the EFI boot entry.\n\n[Secure Boot setup with systemd-boot - Debian Wiki](https://wiki.debian.org/SecureBoot#Secure_Boot_setup_with_systemd-boot)\n\n> shim is a simple software package that is designed to work as a first-stage bootloader on UEFI systems.\n>\n> It was developed by a group of Linux developers from various distros, working together to make Secure Boot work using Free Software. It is a common piece of code that is safe, well-understood and audited so that it can be trusted and signed using platform keys. This means that Microsoft (or other potential firmware CA providers) only have to worry about signing shim, and not all of the other programs that distro vendors might want to support.\n\n[Shim - Debian Wiki](https://wiki.debian.org/SecureBoot#Shim)\n\nIn other words, the default secure boot keys on your motherboard firmware that work for Windows secure boot will also work for the shim secure boot. This makes it so you don't have to configure custom, non-standard secure boot keys in your motherboard firmware (UEFI).\n\nNote... shim is not a security bypass that allows secure booting of any arbitrary unsigned EFI. It \"Enforces signature verification on whatever it chainloads\".\n\n```\nUEFI firmware\n  -> shimx64.efi (Microsoft-signed shim)\n     -> verifies next EFI binary (systemd-bootx64.efi, grubx64.efi)\n        -> launches it if trusted (Linux kernel)\n```\n\n## How I Did It\n\n**Follow the instructions: [Secure Boot setup with systemd-boot - Debian Wiki](https://wiki.debian.org/SecureBoot#Secure_Boot_setup_with_systemd-boot)**\n\nIn the sections that follow, I'll explain where I found the docs lacking, and more detail on what I did.\n\n### Why It Was Not Working (Insofar as I Can Tell)\n\nI tried following the instructions outlined in the Debian Wiki, and I found them difficult to understand.\n\nI suspect there's one of two reasons it didn't work for me initially...\n\n#### Reason 1: Missing `systemd-boot-efi-[amd64|arm64]-signed`\n\nI did not first install `systemd-boot-efi-[amd64|arm64]-signed` as instructed:\n\n> If GRUB packages are not installed, and both the shim-signed and systemd-boot-efi-[amd64|arm64]-signed packages are installed, then the maintainer scripts will run logic to install the appropriate EFI binaries to the ESP...\n\nI'm not really sure why the command to install this is not explicitly included with the other `apt` commands in the doc's code blocks. But I addressed this with:\n\n```bash\nsudo apt install --reinstall shim-signed systemd-boot-efi-amd64-signed\n```\n\n#### Reason 2: Leftover GRUB\n\nMaybe I did not do a thorough enough purging of leftover GRUB?\n\n> A user wanting to switch from GRUB to systemd-boot should do so by removing the GRUB packages and installing systemd-boot at the same time, for example on an amd64 system:\n> `# apt install --allow-remove-essential systemd-boot grub-efi-amd64-signed-`\n\nFor good measure I did the following to fully remove GRUB:\n\n```bash\n# Find all grub packages\napt list --installed | grep -i grub\n# Remove em. Your packages may vary, these were mine.\nsudo apt purge grub-common grub-efi-amd64-bin grub-efi-amd64-unsigned grub-efi-amd64 grub2-common\n# Other grub artifacts removed\n# Enter root session `sudo -i` and `cd /boot` partition\nfind . -iname '*grub*'\nrm -r ./grub/\n```\n\n## Additional Info\n\n### Cleaning Up Pre-Shim systemd-boot EFIs\n\nWhen I reinstalled one of the systemd-boot packages (probably `systemd-boot`, don't remember), I got a message about these files already being present:\n\n- `/boot/efi/EFI/systemd/systemd-bootx64.efi`\n- `/boot/efi/EFI/BOOT/BOOTX64.EFI`\n\nOut of an abundance of caution, I first removed the `.efi` files (perhaps you should back them up somewhere):\n```bash\nsudo rm /boot/efi/EFI/systemd/systemd-bootx64.efi /boot/efi/EFI/BOOT/BOOTX64.EFI\n```\n\nAnd then I reinstalled said systemd-boot packages, just to be sure that the `.efi` files were signed, postinst scripts ran correctly, etc.\n\n### Commands for: `efibootmgr - change the UEFI Boot Manager configuration`\n\nSee: `man efibootmgr`\n\nUsing `efibootmgr -v`, you can see the UEFI boot entries (`Boot0001`, `Boot0002`, etc.).\n\nYou can supposedly delete boot entries through this interface, but this didn't work for me. I think this only deactivated/disabled boot entries: `sudo efibootmgr -b 0004 -B`\n\nAlternatively you can disable unwanted entries in your motherboard's UEFI menu.\n\n### Set Default systemd-boot Entry to Current\n\nOnce you have booted into the right systemd-boot entry:\n\n```bash\nsudo bootctl set-default @current\n```\n\n### Verify Secure Boot Enabled From Linux\n\n```bash\n$ sudo mokutil --sb-state\nSecureBoot enabled\n```\n\n### Get Info From `bootctl status`\n\n`sudo bootctl status` ouputs lots of info such as:\n\n```\nSystem:\n      Firmware: UEFI 2.70 (American Megatrends 5.19)\n Firmware Arch: x64\n   Secure Boot: enabled (user)\n  TPM2 Support: yes\n  Measured UKI: no\n  Boot into FW: supported\n\n...\n\nAvailable Boot Loaders on ESP:\n          ESP: /boot/efi (/dev/disk/by-partuuid/d64202ea-8de9-47d6-947c-11122d0ba571)\n         File: ├─/EFI/systemd/systemd-bootx64.efi (systemd-boot 257.9-1~deb13u1)\n               ├─/EFI/BOOT/BOOTX64.efi\n               └─/EFI/BOOT/fbx64.efi\n\n...\n\nBoot Loaders Listed in EFI Variables:\n        Title: Debian\n           ID: 0x0003\n       Status: active, boot-order\n    Partition: /dev/disk/by-partuuid/d64202ea-8de9-47d6-947c-11122d0ba571\n         File: └─EFI/DEBIAN/SHIMX64.EFI\n\n... and more!\n```\n\n"},{"title":"Hitting a Target File Size With FFmpeg (Perfect for Discord Clips)","date":"2025-12-10T12:43:16.627Z","slug":"hitting-a-target-file-size-with-ffmpeg-perfect-for-discord-clips","content":"\nIf you've ever tried to send a gaming clip over Discord as a free user, you've run into the 10MB upload limit. My process before creating this script was to use a collection of other ffmpeg commands to trial-and-error trimming the clip, and lowering the quality until it finally went below the 10MB threshold.\n\nI wanted a way to say: **\"Here's my clip. Make it 10MB. Do what you must.\"**\n\nSo I wrote a Bash function that uses FFmpeg's 2‑pass encoding method to reliably target a specific file size. It also has quality‑of‑life options for trimming timestamps. As you'll see below, you can write other bash functions which call the main function to make this process happen by typing just a few characters into your terminal.\n\nMy setup:\n\n- Gaming on Windows, running the script in WSL (Git Bash will work too)\n- Shadowplay 1 minute instant replay saved videos\n- Discord 10MB file upload limit (non-Nitro paid users)\n\n## Requirements\n- A Bash shell to run the script: Linux, WSL, or Git Bash\n- FFmpeg (should include `ffmpeg` & `ffprobe`)\n\n**NOTE** I have FFmpeg installed in my Linux distro mounted in WSL (more on that [here](/blog/accessing-a-dual-boot-linux-install-in-wsl-with-chroot)), but using WSL with FFmpeg installed in Windows, with the exe accessible in your `$PATH` should work too.\n\n## The Scripts\n\nAlso available as a [GitHub Gist](https://gist.github.com/zzzachzzz/1a1e056160c5fd79262bf702022cfe59)\n\n### `vidunderfilesize.sh`\n\n```bash\n#!/usr/bin/env bash\n\neval set -- \"$(\n  getopt \\\n    --options s:e:i:o:k:a: \\\n    --longoptions start:,end:,input:,output:,output-filesize-kb:,audio-bitrate-kbps: \\\n    -- \"$@\"\n)\"\n\nwhile true; do\n  case \"$1\" in\n    -s|--start) start_arg=\"$2\"; shift 2 ;;\n    -e|--end) end_arg=\"$2\"; shift 2 ;;\n    -i|--input) input=\"$2\"; shift 2 ;;\n    -o|--output) output=\"$2\"; shift 2 ;;\n    -k|--output-filesize-kb) output_filesize_kb=\"$2\"; shift 2 ;;\n    -a|--audio-bitrate-kbps) audio_bitrate_kbps=\"$2\"; shift 2 ;;\n    --) shift; break ;;\n  esac\ndone\n\nIFS=\":\" read -ra start_array <<< \"$start_arg\"\nIFS=\":\" read -ra end_array <<< \"$end_arg\"\n\n# Prepend default zeros, filling arrays to 3 slots\nwhile (( ${#start_array[@]} < 3 )); do\n  start_array=(0 ${start_array[@]})\ndone\nwhile (( ${#end_array[@]} < 3 )); do\n  end_array=(0 ${end_array[@]})\ndone\n\n# Pad left 2 digits for each slot\nfor i in \"${!start_array[@]}\"; do\n  start_array[$i]=$(printf \"%02d\" ${start_array[$i]})\ndone\nfor i in \"${!end_array[@]}\"; do\n  end_array[$i]=$(printf \"%02d\" ${end_array[$i]})\ndone\n\nstart_hours_part=${start_array[0]}\nstart_minutes_part=${start_array[1]}\nstart_seconds_part=${start_array[2]}\nstart_seconds=$(( $start_seconds_part + $start_minutes_part * 60 + $start_hours_part * 60 * 60 ))\nstart_time=\"$start_hours_part:$start_minutes_part:$start_seconds_part\"\n\n# If no end timestamp supplied (or user provided zeroes), get duration of video\nif [[ \"${end_array[@]}\" == \"00 00 00\" ]]; then\n  raw_duration_decimal=\"$(\n    ffprobe -v error -select_streams v:0 -show_entries stream=duration \\\n    -of default=noprint_wrappers=1:nokey=1 \"$input\"\n  )\"\n  end_seconds=\"$(awk -v d=\"$raw_duration_decimal\" 'BEGIN {print int(d)}')\"\n  # `end_time` intentionally not set\nelse\n  end_hours_part=${end_array[0]}\n  end_minutes_part=${end_array[1]}\n  end_seconds_part=${end_array[2]}\n  end_seconds=$(( $end_seconds_part + $end_minutes_part * 60 + $end_hours_part * 60 * 60 ))\n  end_time=\"$end_hours_part:$end_minutes_part:$end_seconds_part\"\nfi\n\nduration_seconds=$(( $end_seconds - $start_seconds ))\n\ndefault_audio_bitrate_kbps=\"128\"\naudio_bitrate_kbps=\"${audio_bitrate_kbps:-$default_audio_bitrate_kbps}\"\naudio_total_kb=$(( $audio_bitrate_kbps * $duration_seconds ))\n\n_10MB_in_kb=\"80000\" # 10 megabyte == 80,000 kilobits\noutput_filesize_kb=${output_filesize_kb:-$_10MB_in_kb}\n\nvideo_bitrate_kbps=$(( ( $output_filesize_kb - $audio_total_kb ) / $duration_seconds ))\n\n# tmp file written to in 1st pass and read in 2nd pass\nffmpeg_stats_file=\"$(mktemp --tmpdir --dry-run ffmpeg_stats_XXXXXX)\"\n\nmaybe_to=\"$([[ -n $end_time ]] && echo \"-to $end_time\")\"\nffmpeg_common_output_file_options=\"\\\n-ss $start_time $maybe_to \\\n-c:v libx264 -b:v ${video_bitrate_kbps}k \\\n-maxrate 5M -bufsize 10M \\\n-passlogfile $ffmpeg_stats_file \\\n$FFMPEG_OUTPUT_FILE_OPTIONS\\\n\"\n\n# 1st pass ffmpeg analyzes video and outputs stats file to `$ffmpeg_stats_file*`,\n# to accurately target `$output_filesize_kb` based on specified bitrates.\nffmpeg -y -i \"$input\" \\\n  $ffmpeg_common_output_file_options \\\n  -pass 1 \\\n  -an -f mp4 \\\n  /dev/null\n\n# 2nd pass produces actual video output, reading from stats files produced in 1st pass\nffmpeg -i \"$input\" \\\n  $ffmpeg_common_output_file_options \\\n  -pass 2 \\\n  -c:a aac -b:a \"${audio_bitrate_kbps}k\" \\\n  \"$output\"\n\n# Clean up tmp files, mbtree file is ~20MB\nrm \"$ffmpeg_stats_file\"*.log{,.mbtree}\n\n```\n\nAdditionally, here is my own helper function I use for calling the main script. I suggest you create one too.\n\n```bash\nfunction meleeclip() {\n  local start=\"$1\"\n  local end=\"$2\"\n  local input=\"$3\"\n  local output=\"$4\"\n  # Defaults for empty start & end options handled in `vidunderfilesize.sh`\n  [[ \"$start\" == \"-\" ]] && start=\"\"\n  [[ \"$end\" == \"-\" ]] && end=\"\"\n\n  if [[ -z \"$input\" || \"$input\" == \"-\" ]]; then\n    local input_dir=\"/mnt/d/storage/Videos/shadowplay/slippi dolphin.exe\"\n    local newest_file=\"$(ls -tp \"$input_dir\" | grep -v \"/$\" | head -n 1)\"\n    input=\"${input_dir}/${newest_file}\"\n  fi\n\n  if [[ -z \"$output\" || \"$output\" == \"-\" ]]; then\n    local output_dir=\"/mnt/d/storage/Videos/shadowplay/slippi dolphin.exe/cropped-clips\"\n    local output_filename=\"_NEW_CLIP_NAME_ME_$(basename \"$input\")\"\n    output=\"${output_dir}/${output_filename}\"\n  fi\n\n  FFMPEG_OUTPUT_FILE_OPTIONS=\"-vf crop=in_h*4/3:in_h\" \\\n    ~/bin/vidunderfilesize.sh \\\n    --start=\"$start\" \\\n    --end=\"$end\" \\\n    --input=\"$input\" \\\n    --output=\"$output\"\n}\n```\n\n## Usage\n\n`FFMPEG_OUTPUT_FILE_OPTIONS` is an option you probably don't need. Here I am cropping to 4:3 aspect ratio to closely match [Slippi Dolphin Melee's](https://melee.tv) internal resolution of 73:60.\n\nThe `--start` and `--end` options allow you to specify the timestamps you want to trim the clip to. The parsing is flexible and allows for these example inputs, converting to `hh:mm:ss`.\n\n- `5` -> `00:00:05`\n- `1:50` -> `00:01:50`\n- `01:1:01` -> `01:01:01`\n\nWith the `meleeclip` helper function I have, this is how I would typically call it:\n\n```bash\n# Let's say we want to clip from 00:00:36 to the end of my 1 minute clip\nmeleeclip 36\n\n# Or from 00:00:36 to 00:00:50\nmeleeclip 36 50\n```\n\nThat's it, since I have the `meleeclip` function default to use the newest file in my shadowplay directory as input, and a fixed output directory with a generated filename that I just go and rename separately after.\n\n## Explanation\n\nBitrate is predictable for producing a given filesize. File size is basically:\n\n```\nfile_size_bits ≈ bitrate_bits_per_sec × duration_sec\n```\n\nSo if you want a 10MB clip:\n\n```\n10 MB (Megabytes) = 80 Mb (megabits)\n10 MB = 80000 Kb (kilobits)\ntarget_bitrate_kbps = 80000 Kb / duration_seconds\n```\n\nIn the script, we calculate the maximum video bitrate we can use:\n```bash\nvideo_bitrate_kbps=$(( ( $output_filesize_kb - $audio_total_kb ) / $duration_seconds ))\n```\n\nOnce you know the exact bitrate needed, FFmpeg can aim for it, and using **2‑pass encoding**, it can hit that target very accurately.\n\n"},{"title":"Accessing a dual boot Linux install in WSL with chroot","date":"2024-10-11T14:49:56.465Z","slug":"accessing-a-dual-boot-linux-install-in-wsl-with-chroot","content":"\nGot a Windows & Linux dual boot setup? Rather than setting up a new WSL install, wouldn't it be nice to be able to \"import\" your existing native Linux install into WSL? While this method is not exactly an import, it is functionally similar.\n\n## Mounting the disk partition\n\nResources\n- [WSL 2 - Mounting a partitioned disk](https://learn.microsoft.com/en-us/windows/wsl/wsl2-mount-disk#mounting-a-partitioned-disk)\n- [EXT4 in Windows - Chris Titus Tech](https://www.youtube.com/watch?v=aX1vH1j7m7U)\n\nThe first step is mounting the partition containing your Linux install. If you only want to access the files of an ext4 partition from Windows, you'll only need this step.\n\nI have defined a couple of PowerShell functions to making mounting and unmounting easier:\n\n```powershell\n# These WSL (un)mount commands must be run in PowerShell as administrator\n\n# TODO: Replace the Model string with your own value: 'SHGS31-500GS-2'\n# TODO: Replace the parition number with your own value: 4\n\nFunction linuxmnt { wsl --mount (gcim -query \"SELECT * from Win32_DiskDrive WHERE Model = 'SHGS31-500GS-2'\" | select -expandproperty DeviceID) --partition 4 }\n\nFunction linuxunmnt { wsl --unmount (gcim -query \"SELECT * from Win32_DiskDrive WHERE Model = 'SHGS31-500GS-2'\" | select -expandproperty DeviceID) }\n```\n\nThe `linuxmnt` function will mount partition 4 of drive 0 in WSL at the mountpoint `/mnt/wsl/PHYSICALDRIVE0p4`. This path will vary based on the drive/device id and partition number you are targeting. The `linuxunmnt` will simply unmount all mountpoints for that drive.\n\nYou can include these in your PowerShell profile so they are always available. Your profile file path can be found with `echo $profile`, which will likely output one of the two following paths, depending on the version of PowerShell you are running:\n- `C:\\Users\\<your-user>\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1`\n- `C:\\Users\\<your-user>\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1`\n\nYou'll need to modify the `linuxmnt` and `linuxunmnt` functions, as I have hardcoded the model (to choose the correct disk) and partition number I need to target.\n\n### Determining the device id\n\nBegin by querying for this information in PowerShell:\n\n```powershell\n> gcim -query \"SELECT * from Win32_DiskDrive\"\nDeviceID           Caption                 Partitions Size          Model\n--------           -------                 ---------- ----          -----\n\\\\.\\PHYSICALDRIVE0 SHGS31-500GS-2          3          500105249280  SHGS31-500GS-2\n\\\\.\\PHYSICALDRIVE1 WDC WD1003FZEX-00K3CA0  1          1000202273280 WDC WD1003FZEX-00K3CA0\n\\\\.\\PHYSICALDRIVE2 ST2000DM008-2FR102      1          2000396321280 ST2000DM008-2FR102\n\\\\.\\PHYSICALDRIVE3 Samsung SSD 990 EVO 1TB 3          1000202273280 Samsung SSD 990 EVO 1TB\n```\n\nWe target the `Model` attribute rather than `DeviceID`, since the physical drive number can change if hardware is added or removed. Although, in the bash function below we hardcode the path `/mnt/wsl/PHYSICALDRIVE0p4`, physical drive 0. Something to be aware of, which could technically be made dynamic.\n\n### Determining the partition number\n\nTo determine the partition number containing the Linux filesystem, we can start with a bare mount, which will make all the drive's partitions visible in WSL, but without a mount point.\n\n```powershell\n# Run in PowerShell as administrator\nwsl --mount \\\\.\\PHYSICALDRIVEX --bare # Specify correct id instead of X\n```\n\nAfter the WSL bare mount, run `lsblk` from WSL.\n\n```\n$ lsblk\nNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nsda      8:0    0 388.4M  1 disk\nsdb      8:16   0     2G  0 disk [SWAP]\nsdc      8:32   0 465.8G  0 disk\n├─sdc1   8:33   0   512M  0 part\n├─sdc3   8:35   0   3.7G  0 part\n└─sdc4   8:36   0 461.5G  0 part\nsdd      8:48   0     1T  0 disk /mnt/wslg/distro\n                                 /\n```\n\nBased on the partition sizes, I can see that `sdc4` is what I'm looking for, the main ext4 filesystem. If you want to inspect a partition further to verify, you can use `mount` in the usual way to mount its contents to a directory.\n\n## Chroot-ing into the Linux install\n\nThe helper function (named walter because that's the hostname that was chosen for the original native install):\n\n```bash\n# TODO: Replace PHYSICALDRIVE0p4 with the correct value based on the device id and partition number mounted\n# TODO: Replace zach with the user you wish to switch to\n\nfunction walter() {\n  if [[ ! -e /mnt/wsl/PHYSICALDRIVE0p4 ]]; then\n    echo \"'/mnt/wsl/PHYSICALDRIVE0p4' does not exist. Did you call linuxmnt?\"\n    return 1\n  fi\n  sudo mount -t proc /proc /mnt/wsl/PHYSICALDRIVE0p4/proc\n  sudo mount -t sysfs /sys /mnt/wsl/PHYSICALDRIVE0p4/sys 2> /dev/null\n\n  sudo mount --rbind /dev /mnt/wsl/PHYSICALDRIVE0p4/dev\n  sudo mount --rbind /mnt /mnt/wsl/PHYSICALDRIVE0p4/mnt\n  sudo chroot /mnt/wsl/PHYSICALDRIVE0p4/ su zach\n}\n```\n\nAdd the above function to your `.bashrc` or other config file in your WSL install.\n\nRandom note, in my Debian WSL install, when I added a function to the `.bashrc` file, running `which walter` did not find the function, but `type walter` did. More on that [here](https://unix.stackexchange.com/a/10529).\n\nAt a minimum, all that's really needed is this line, `sudo chroot /mnt/wsl/PHYSICALDRIVE0p4/ su zach`, to choot into the mountpoint and switch from the root user to your regular user.\n\n`sudo mount --rbind /mnt /mnt/wsl/PHYSICALDRIVE0p4/mnt` - This is helpful so that the typical WSL mounts to access the Windows C:\\ drive (`/mnt/c`) and any additional drives are accessible.\n\nThe `/proc`, `/sys`, and `/dev` mounts are necessary for certain programs to work. In my case for `tmux`.\n\n## Bringing it all together: The Workflow\n\n1. Open PowerShell as Administrator and run `linuxmnt`\n2. Open a terminal for your WSL install, and run `walter` (or whatever you chose to name it).\n3. You have now chroot'd into your non-WSL Linux install! You'll have access to that shell and your installed programs, kind of as if you had SSH'd into it.\n\n"},{"title":"How to Change a Pacman Package's Dependencies on Arch Linux with remakepkg","date":"2022-10-22T20:13:29.318Z","slug":"how-to-change-a-pacman-packages-dependencies-on-arch-linux-with-remakepkg","content":"\nRecently I ran into a dependency conflict after an update of a particular package, [bitwarden-cli](https://archlinux.org/packages/community/any/bitwarden-cli/). Prior to version 2022.6.2-2, `bitwarden-cli` depended on `nodejs`. It was in version 2022.6.2-2 that the dependency was changed from `nodejs` to `nodejs-lts-gallium`.\n\nOkay, so what's the big deal? The big deal is that `nodejs` and `nodejs-lts-gallium` cannot both be installed, as they're listed as conflicting packages of one another (see the **Conflicts** section of [nodejs-lts-gallium](https://archlinux.org/packages/community/x86_64/nodejs-lts-gallium/)). While I could have just gone ahead and removed `nodejs` in favor of `nodejs-lts-gallium`, I didn't really want to, as I had no issues with it and wanted the latest version of `nodejs`. And yes, alternatively I could have installed the latest version via [nvm](https://github.com/nvm-sh/nvm) or otherwise, and kept `nodejs-lts-gallium` (version 16.x at the time of this writing) as the system dependency.\n\nFor whatever your personal reasons are, you may wish to remove or alter the restrictions defined for a package by a package maintainer. In my case, I was able to figure out why the dependency change occurred in the first place. It was due to [this bug](https://bugs.archlinux.org/task/74929), a bug which affected a premium feature of Bitwarden that I didn't even have access to. With this in mind, I felt comfortable discarding the new requirement of using `nodejs-lts-gallium`.\n\n**IMPORTANT NOTE**  \nIn writing this, I realized that `--assume-installed nodejs-lts-gallium` solves my problem, without the need for `remakepkg`. I just need to include that option during an upgrade, whenever there is a newer version of `bitwarden-cli` available. This works when dealing with the absence of a package. However, `remakepkg` may be needed for other scenarios. I'll proceed with how I used `remakepkg` to solve my problem.\n\nSo, how does one go about \"changing\" a package's dependency? It's done with the help of an AUR package, [remakepkg](https://aur.archlinux.org/packages/remakepkg). You can also find the git repo containing the script [here](https://gitlab.com/ayekat/pacman-hacks).\n\nFirst off, credit to this comment from the forum post, [nodejs-lts-gallium and nodejs are in conflict](https://bbs.archlinux.org/viewtopic.php?pid=2030394#p2030394). I owe the entirety of my solution to this comment. However for the non pacman experts, I'll provide some more details on how I solved my problem.\n\nPrior to discovering this solution, I had added the line `IgnorePkg = bitwarden-cli` to my `/etc/pacman.conf` file, preventing an upgrade of this package that introduced the `nodejs-lts-gallium` dependency. The problem is that upon removing this, and running `pacman -Syu`, I was prompted to remove the conflicting package `nodejs`, with the upgrade of `bitwarden-cli`. I needed to get the latest version of `bitwarden-cli`, but not actually install it until I had made the necessary dependency modifications. This is how I achieved that:\n\n```bash\npacman -S --downloadonly --assume-installed nodejs-lts-gallium bitwarden-cli\n```\n\nThere is another, less manual way to accomplish this. If you see the [author's guide to remakepkg](https://bbs.archlinux.org/viewtopic.php?id=234936), you'll see that the `remakepkg` command has the added convenience of downloading the latest version of a package from a mirror, independent of your system's main synced package database. More details on that [here](https://gitlab.com/ayekat/pacman-hacks/-/issues/43).\n\nMoving on with how I did this the manual way, after running the above pacman command to download `bitwarden-cli`, I was then able to access the downloaded package file from `/var/cache/pacman/pkg/bitwarden-cli-2022.10.0-1-any.pkg.tar.zst`. I copied this to another location to prepare a modified copy. Next, I created a file `rulefile`:\n```bash\nremove-depend nodejs-lts-gallium\nadd-depend nodejs\n```\nand supplied it to the `repkg` command:\n```bash\nrepkg -i ./bitwarden-cli-2022.10.0-1-any.pkg.tar.zst -r ./rulefile\n```\nThe `repkg` command produced an output modified package file:  \n`bitwarden-cli-2022.10.0-1.1-any.pkg.tar.xz`\n\nNotice the appended `.1` to the version info, and the alternate `xz` extension for the compression format.\n\nRunning `pacman -Qpi ./bitwarden-cli-2022.10.0-1.1-any.pkg.tar.xz`, we see that the dependency was replaced in the \"Depends On\" section:\n```\nName            : bitwarden-cli\nVersion         : 2022.10.0-1.1\nDescription     : The command line vault\nArchitecture    : any\nURL             : https://github.com/bitwarden/cli\nLicenses        : GPL3\nGroups          : None\nProvides        : bitwarden-cli=2022.10.0-1\nDepends On      : nodejs\n...\n```\n\nAll that's left now is to install the modified package:\n```bash\npacman -U ./bitwarden-cli-2022.10.0-1.1-any.pkg.tar.xz\n```\nYou can verify the installed version once again with `pacman -Qi bitwarden-cli`\n\nThat's it! As I mentioned above, `remakepkg` isn't necessary for my scenario, as pacman's `--assume-installed` option does the trick. However for more advanced dependency scenarios, `remakepkg` can come in handy.\n\n"},{"title":"A Curl Helper Function for Easy API Testing","date":"2021-09-06T18:23:42.735Z","slug":"a-curl-helper-function-for-easy-api-testing","content":"\nI tend to prefer command line tools for development, in this case choosing `curl` over Postman. Depending on the API that requests are being made to, curl commands can get out of hand, requiring numerous headers and other options to be manually attached on each request. The solution? Create a Bash helper function for curl, making our commands short and efficient.\n\nThe helper function:\n\n```bash\nfunction curls() {\n  local response_code_and_method\n  response_code_and_method=$(curl \\\n    --no-progress-meter \\\n    --write-out \"%{response_code} %{method}\" \\\n    --output /tmp/curls_body \\\n    --header \"Content-Type: application/json\" \\\n    ${CURL_OPTIONS[@]} \\\n    $CURL_BASE_URL/$@\n  )\n\n  if [ $? -eq 0 ]; then\n    local pretty_json\n    pretty_json=$(jq --color-output '.' /tmp/curls_body 2> /dev/null)\n    if [ $? -eq 0 ]; then\n      echo $pretty_json\n    else\n      cat /tmp/curls_body\n      echo \"\"\n    fi\n    echo \"\\n$response_code_and_method\"\n  fi\n}\n```\n\nIn addition to providing a handful of \"default\" options to `curl`, we get some other benefits including:\n* Pretty printing JSON responses with `jq` (conditionally, when a response is parse-able as JSON)\n* Using `${CURL_OPTIONS[@]}`, we can provide additional options through an environment variable. This may be preferred for temporarily adding additional options, rather than hard-coding them in our reusable Bash function.  \nFurther details on this option are shown below: [Additional Notes](#additional-notes)\n* On the line `$CURL_BASE_URL/$@`, a variable representing the API's base URL is automatically inserted for us. This would be set to a value such as `http://localhost:5000`. A trailing slash `/` is appended, and our arguments provided to `curls` are inserted here with `$@`.\n\n## Example Usage\n\nCalling the endpoint `POST http://localhost:5000/test` with a request body can concisely be expressed as:\n\n`curls test -d '{\"key\": \"value\"}'`\n\nFull example with a minimal Flask app:\n\n![Full example with a minimal Flask app](/assets/curls-flask.png)\n\n## Additional Notes\n\n* Pretty printing JSON requires installing [jq](https://stedolan.github.io/jq/) (available from `brew`)\n* The `%{method}` formatter for the `--write-out` option requires curl 7.72.0+. <https://curl.se/changes.html#7_72_0>  \nThe easiest way to install the latest version is `brew install curl` (OSX & Linuxbrew). For me on Ubuntu, the version available via `apt` was an older version.\n\n* To get a better understanding of the Bash function and to modify it to your liking, I recommend this Bash cheatsheet: <https://devhints.io/bash>\n\n* Further explaining `${CURL_OPTIONS[@]}`, `CURL_OPTIONS` should be an array, whose elements are then expanded and space separated with `[@]`. For example, you may want to add another header containing your access token or API key.  \nExample: `CURL_OPTIONS=(\"--header\" \"Authorization: Bearer <your-access-token>\")`\n\n"},{"title":"React Hooks: How to Use useMemo","date":"2021-03-05T21:29:51.159Z","slug":"react-hooks-how-to-use-usememo","content":"\n>\"In computing, memoization or memoisation is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again.\"  \n[Memoization - Wikipedia](https://en.wikipedia.org/wiki/Memoization)\n\n`useMemo` is a hook used to memoize values inside a React component. It's a performance optimization to avoid recalculating expensive values on every render. You might be familiar with React's `memo` function, which is similar, but is used to memoize React components themselves, to avoid said re-renders in the first place.\n\nThe TypeScript function signature of `useMemo`:\n\n```ts\ntype useMemo = <T>(factory: () => T, deps: Array<any>) => T;\n```\n\nThe first argument is a factory function returning the value we want to memoize. Like `useEffect` and `useCallback`, the second argument to this hook, `deps`, is a dependency array. Changes to the values passed to this array will trigger our factory function to rerun, returning a new value. If the values in the dependency array do not change, we'll instead receive the memoized value saved during the most recent execution of the factory function.\n\n## Example\n```jsx\nfunction Todos({ todos, filterByStatus }) {\n  const filteredTodos = useMemo(() => {\n    return todos.filter(todo => todo.status === filterByStatus);\n  }, [todos, filterByStatus]);\n\n  return (\n    <ul>\n      {filteredTodos.map((todo, i) =>\n        <li key={i}>{todo.name}</li>\n      )}\n    </ul>\n  );\n}\n```\n\nWe receive the props `todos`, an array of todo objects, and `filterByStatus`, a string indicating the status we want to filter by, such as 'completed', 'in-progress', etc. If todo objects are added or removed from the `todos` array, that will affect the resulting `filteredTodos`. Likewise, the result will change if our `filterByStatus` changes from 'in-progress' to 'completed'. We include both of these variables in the dependency array to signal to `useMemo` that changes to these variables should trigger a recalculation of our computed `filteredTodos` value.\n\nNote that for `useMemo` to detect that the `todos` array is unchanged between renders, it must be equal by reference. The variables included in the dependency array will be compared to their previous values using strict equality (`===`). Remember that arrays, objects, and functions are only equal by reference:\n```js\n[1, 2, 3] === [1, 2, 3] // false\nlet x = [1, 2, 3];\nx === x // true;\n```\n\nUsing our example with the `todos` array prop, if that prop comes from a parent component storing the value as state:\n```js\nconst [todos, setTodos] = useState([]);\n```\n...then `todos` will remain referentially equal _until_ `setTodos` is called. You can test this out for yourself, by placing a `console.log` statement inside your `useMemo`'s factory function, to see when it is triggered.\n\n`useMemo` has a lot of concepts in common with `useEffect` and `useCallback`, and yet, at least to me, `useMemo` is a lot easier to understand. Perhaps that's because it seems less intuitive to memoize functions, in the case of `useEffect` and `useCallback`.\n\nFor a more detailed explanation of the dependency array, and other related concepts, you can check out:\n* My post on useEffect: [React Hooks: How to Use useEffect](/blog/react-hooks-how-to-use-useeffect)\n* [Object & array dependencies in the React useEffect Hook](https://www.benmvp.com/blog/object-array-dependencies-react-useEffect-hook)\n\n"},{"title":"React Hooks: How to Use useEffect","date":"2021-03-04T18:04:42.446Z","slug":"react-hooks-how-to-use-useeffect","content":"\nOf all the hooks built into React, `useEffect` is arguably the most difficult to understand. When I was learning React Hooks, I had just begun to get comfortable with class-based components and the lifecycle methods, such as `componentDidMount`. Part of the difficulty I had when learning `useEffect` was due to the fundamental differences between `useEffect` and the legacy React lifecycle methods. The best tutorials I've read on `useEffect` advise you to \"unlearn what you have learned\" in regard to lifecycle methods.\n\n[Dan Abramov has an excellent blog post on useEffect](https://overreacted.io/a-complete-guide-to-useeffect). It's very thorough, and thus a long read. This post will summarize many of the points Dan covers, and I'll cover some of the issues and solutions I've discovered while using `useEffect`.\n\nFirst, here is the function signature for `useEffect` as a TypeScript definition:\n\n```ts\ntype useEffect = (effect: EffectCallback, deps?: Array<any>) => void;\ntype EffectCallback = () => (void | (() => void));\n```\n\n`EffectCallback` is our function to execute as the effect, which can optionally return a [cleanup function](https://reactjs.org/docs/hooks-reference.html#cleaning-up-an-effect) that will be executed when the component unmounts, or when the effect is redefined. The optional second argument to `useEffect`, `deps`, is a \"dependency array\". If `deps` is omitted, then the effect will be executed (and redefined) after every render. When `deps` is included, the effect is only redefined and executed if any of the values provided to the array change from one execution to the next. Consequently, providing no values to the dependency array, `[]`, will result in the effect only being executed after the initial render. In determining if a dependency has changed, as far as I know, a strict equality comparison is performed (`===`). Note that arrays, objects, and functions are only equal by reference. In some situations this can be problematic. This blog post provides several solutions:  \n[Object & array dependencies in the React useEffect Hook](https://www.benmvp.com/blog/object-array-dependencies-react-useEffect-hook)\n\nWhy is it even necessary to have a dependency array? How could we be accessing stale values inside an effect?\n\nConsider the following snippet of vanilla JS:\n```js\nlet arr = [];\nlet y = 0;\n\nfunction pushFunc() {\n  y++;\n  let x = y;\n  arr.push(() => x);\n}\n\npushFunc();\npushFunc();\n\nconsole.log(arr[0]()); // 1\nconsole.log(arr[1]()); // 2\n```\nWe push two functions to an array, `() => x`, and each time this function is created, it captures `x` from its [closure](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Closures) within `pushFunc`. `x` from the first execution of `pushFunc` is not the same `x` in the second execution of `pushFunc`. When dealing with React, the same rules apply, whether those values come from props or state, as they're also just variables. This is because a React component is just a function, and plays by the same rules concerning function execution context.  \n1 render = 1 function call.\n\nIf we were to provide a function to `useEffect` with no dependency array; `useEffect(() => {...})`, the effect function we provide would be redefined after every render, receiving fresh values from the current execution context. The effect would also re-execute after every render. The dependency array serves two purposes:\n1. Tell React when to execute our effect\n2. Tell React when to redefine our effect\n\n## Example 1: Basic Usage with `fetch`\nThe most common use case for `useEffect` is fetching data from an API, and then updating the state of a component to render that data in the UI.\n\n```jsx\nfunction Todo({ id }) {\n  const [todo, setTodo] = useState();\n\n  useEffect(() => {\n    fetch(`/api/todos/${id}`)\n      .then(res => res.json())\n      .then(json => setTodo(json));\n  }, []);\n\n  if (!todo) return null;\n  return <div>{todo.title}</div>;\n}\n```\n\nWhile we should avoid making too many comparisons to the class lifecycle methods, the above usage of `useEffect` with an empty dependency array `[]` is the rough equivalent of `componentDidMount`. The above does work in its current form, but we're lying to React about the dependency array. Running the above snippet through `eslint` configured with the rule `react-hooks/exhaustive-deps` gives us this warning:\n```\nReact Hook useEffect has a missing dependency: 'id'.\nEither include it or remove the dependency array\n```\nWe can fix this to become:\n\n```jsx\nuseEffect(() => {\n  ...\n}, [id]);\n```\n\nBy providing `id` to the dependency array we are saying:  \n\"Whenever `id` changes, redefine and rerun this effect.\"\n\n`id` may or may not change depending on how the parent component of our `Todo` component gets a todo id, and provides that prop. If our `Todo` component were to receive a different `id` prop, then we probably would want to fetch the todo corresponding to that new id, calling our effect provided to `useEffect` again.\n\nTechnically `setTodo` should be included in the dependency array too. However, since it is a function we get from our `useState` hook, its identity is guaranteed to be stable, so it will never change. Furthermore, **in newer versions of the `react-hooks/exhaustive-deps` rule, the linter won't tell us to include a `useState` `set_` function, nor the `dispatch` function returned by `useReducer`.** It's safe to omit these specific functions from the dependency array. Just not other functions, as we will see in the next section.\n\n## Example 2: Functions as Dependencies\nNext, let's take a look at functions as effect dependencies:\n\n```jsx\nfunction Todo({ id }) {\n  const [todo, setTodo] = useState();\n\n  function fetchTodo() {\n    return fetch(`/api/todos/${id}`);\n  }\n\n  useEffect(() => {\n    fetchTodo()\n      .then(res => res.json())\n      .then(json => setTodo(json));\n  }, []);\n\n  if (!todo) return null;\n  return <div>{todo.title}</div>;\n}\n```\n\nIn this example, our effect calls a function, `fetchTodo`. This code contains a bug. :bug: Because we omit `fetchTodo` from our effect's dependency array, our effect captures only the original definition of `fetchTodo`, and in turn, that instance of `fetchTodo` only captures the initial value of the `id` prop. If `id` changes, our effect will reference the original stale value. Like in the first example, `id` is a dependency we need to inform our effect about. The difference is, we've now made that dependency indirect by accessing `id` inside `fetchTodo` rather than directly inside our effect callback.\n\nThere's a problem with simply adding `fetchTodo` to the dependency array to solve this. Because `fetchTodo` will be redefined on each render / execution of our `Todo` component, `fetchTodo` will have a new \"value\" / \"function identity\" each time, resulting in the effect being triggered on every render. There are two solutions to this problem:\n\n### Solution #1\nInclude `fetchTodo` in the dependency array, and define `fetchTodo` with [useCallback](https://reactjs.org/docs/hooks-reference.html#usecallback). Like `useEffect`, `useCallback` also accepts a dependency array. Because `fetchTodo` references `id` in its function body, we need to include `id` in its dependency array:\n\n```jsx\nconst fetchTodo = useCallback(() => {\n  return fetch(`/api/todos/${id}`);\n}, [id]); // Whenever `id` changes, `fetchTodo` will be redefined\n\nuseEffect(() => {\n  fetchTodo()\n    .then(res => res.json())\n    .then(json => setTodo(json));\n}, [fetchTodo]); // Add `fetchTodo` to the effect's dependency array\n```\n\n### Solution #2\nThe other solution is to extract `fetchTodo` from the component entirely. Being outside the closure of the `Todo` component, it won't have access to the `id` prop, but we can supply that as an argument to the function. Extracting `fetchTodo` will allow its function identity to be stable across renders of `Todo`:\n```jsx\nfunction Todo({ id }) {\n  const [todo, setTodo] = useState();\n\n  useEffect(() => {\n    fetchTodo(id) // Pass `id` as an argument\n      .then(res => res.json())\n      .then(json => setTodo(json));\n  }, [id]); // `fetchTodo` now has a stable function identity\n\n  if (!todo) return null;\n  return <div>{todo.title}</div>;\n}\n\nfunction fetchTodo(id) { // Make `id` an argument\n  return fetch(`/api/todos/${id}`);\n}\n```\n\n## Example 3: Access Updated `props` Without Rerunning an Effect\nLet's look at another example. This one is a fairly unique case, as we need to access updated values in our effect, but re-executing the effect actually breaks the functionality we're going for:\n\n```jsx\nfunction Counter({ incrementBy }) {\n  const [num, setNum] = useState(0);\n\n  useEffect(() => {\n    const handle = setInterval(() => {\n      setNum(num + incrementBy);\n    }, 3000);\n\n    return () => clearInterval(handle);\n  }, [num, incrementBy]);\n\n  return <div>{num}</div>;\n}\n```\n\n`setNum` won't change, but `num` and `incrementBy` are both problematic. With `num` in the dependency array, and our effect updating `num` via `setNum`, this will cause our effect to be triggered every time `setNum(num + incrementBy)` is run. For setting state relying on previous state values, we can use the callback form of `setState`, and remove `num` as a dependency.\n\nIf `num` is omitted from the dependency array, the linter will actually suggest this solution to us:\n\n```\nReact Hook useEffect has a missing dependency: 'num'.\nEither include it or remove the dependency array.\nYou can also do a functional update 'setNum(n => ...)'\nif you only need 'num' in the 'setNum' call\n```\n\nTo use the functional update form of `setState`, we can change this to:\n\n```jsx\nfunction Counter({ incrementBy }) {\n  const [num, setNum] = useState(0);\n\n  useEffect(() => {\n    const handle = setInterval(() => {\n      setNum(prevNum => prevNum + incrementBy); // `num` is no longer used here\n    }, 3000);\n\n    return () => clearInterval(handle);\n  }, [incrementBy]); // `num` removed from dependency array\n\n  return <div>{num}</div>;\n}\n```\n\nNow we're left to deal with `incrementBy`. If this prop is updated, say from `10` to `20`, we do want that updated value to be referenced in our effect, rather than referencing a stale value. However, when our effect is redefined, we lose the timing of our interval, and a new interval is created. We have it setup to call `setNum(prevNum => prevNum + incrementBy)` every 3 seconds.\n\nWhat happens if just 1.5 seconds have passed for the interval, and the value of `incrementBy` changes?\n1. Our effect cleanup function we provided to React will be executed, `() => clearInterval(handle)`, clearing our current 3 second interval.\n2. Our effect will be redefined, creating a new 3 second interval, along with a new cleanup function.\n3. From there, 3 more seconds must pass before `setNum(...)` is called, for a total of 4.5 seconds since the last interval call (wrong behavior).\n\nThis example with `setInterval` provides us with a unique challenge. We want the updated values present in our effect, but we don't want the timing of our interval to be messed up, as a result of redefining our effect. `useReducer` can help us achieve this, by accessing updated props in our reducer function, rather than in `useEffect`:\n```jsx\nfunction Counter({ incrementBy }) {\n  const [num, incrementNum] = useReducer(\n    prevNum => prevNum + incrementBy, // Our \"setter\" (reducer function)\n    0 // Initial state\n  );\n\n  useEffect(() => {\n    const handle = setInterval(() => {\n      incrementNum();\n    }, 3000);\n\n    return () => clearInterval(handle);\n  }, []);\n\n  return <div>{num}</div>;\n}\n```\nWe use `useReducer` in a similar fashion to `useState`, but with the ability to specify what our \"setter\" function does, and for it to access updated props. `useReducer` is flexible in how you use it for your state. It can be used for simple, single value state, or more complex state objects. By convention, you'd normally see `useReducer` used like this: `const [state, dispatch] = useReducer(...)`. We instead choose to name these `num` & `incrementNum`. `incrementNum` is our `dispatch` function that `useReducer` returns to us, and it is guaranteed to have a stable function identity, preventing it from triggering `useEffect` to rerun. Since `incrementNum` is the `dispatch` function returned to us by `useReducer`, it can be omitted from the dependency array and the exhaustive deps linter won't complain.\n\n\n## Conclusion\nHopefully this post helped in understanding `useEffect`. As you can tell, the design of this hook by the React team is something that's opinionated and strict in how it is intended to be used, though that's not a bad thing. Being honest about an effect's dependencies is important in avoiding subtle bugs. We looked at some tricks that can be used to limit the number of dependencies in our effects. These recommended workarounds to reduce dependencies are something I wish was documented a little better in the official React docs. One of the more helpful parts of the docs is the [Hooks FAQ #Performance Optimizations](https://reactjs.org/docs/hooks-faq.html#performance-optimizations) section, which to me seems like more of a general usage guide. Knowing these recommended strategies for working with `useEffect` is crucial, as I've found that it's very easy to \"break the rules\" of `useEffect` when building real world applications.\n"},{"title":"Create a Typed Event Emitter with Native Browser APIs","date":"2021-02-24T19:56:02.926Z","slug":"create-a-typed-event-emitter-with-native-browser-apis","content":"\nYou can create an event emitter in the browser, much like the Node.js [EventEmitter](https://nodejs.dev/learn/the-nodejs-event-emitter) API. We'll be using the [EventTarget](https://developer.mozilla.org/en-US/docs/Web/API/EventTarget) and [CustomEvent](https://developer.mozilla.org/en-US/docs/Web/API/CustomEvent) browser APIs to achieve this. The browser support for these APIs is good, but if you need more browser coverage, there are also polyfills available, such as [custom-event-polyfill](https://www.npmjs.com/package/custom-event-polyfill). As a bonus, we can make the events and their details fully typed with TypeScript.\n\n```ts\nclass EventEmitter extends EventTarget {\n  constructor() {\n    super();\n  }\n\n  on<T extends EventType>(\n    type: T, listener: (e: CustomEvent<EventTypeToDetailMap[T]>) => void\n  ) {\n    return this.addEventListener(type, listener);\n  }\n\n  emit<T extends EventType>(\n    type: T, detail: EventTypeToDetailMap[T]\n  ) {\n    const event = new CustomEvent(type, { detail })\n    return this.dispatchEvent(event);\n  }\n}\n\ntype EventType = keyof EventTypeToDetailMap;\n\ntype EventTypeToDetailMap = {\n  'customEvent1': number;\n  'customEvent2': Array<string>;\n};\n```\n\nAs we write event listeners and emitters for certain events, we get type checking for those specific events:\n\n![type checking for EventEmitter.on](/assets/typed-eventemitter-on.png)\n\n![type checking for EventEmitter.emit](/assets/typed-eventemitter-emit.png)\n\n"},{"title":"How to Set Up WSL for Development","date":"2021-02-23T23:13:25.100Z","slug":"how-to-set-up-wsl-for-development","content":"\nWSL (Windows Subsystem for Linux) is a great way to gain access to a Linux OS through a command line interface. Being restricted to the CLI, WSL does require us to use Windows GUI programs. This, along with WSL being a subsystem that depends on Windows, does result in certain quirks that need to be worked around in order to utilize WSL to the fullest.\n\nSome of these quirks to resolve include:\n* Synchronizing clipboards between WSL & Windows\n* Accessing files from both Windows and WSL\n* Choosing the right terminal to access WSL through\n\nDue to the differences between WSL 1 & 2, the solutions to some of these issues differ depending on the version in use. I'll be focusing mainly on WSL 2 in this post, though I will cover some of the differences between WSL 1 & 2.\n\n## Installation\nIf you haven't already, follow the [installation guide from Microsoft](https://docs.microsoft.com/en-us/windows/wsl/install-win10) to get WSL installed, preferably WSL 2. One possible reason you would need to settle for WSL 1, is if your version of Windows is not new enough, as covered in the guide. Another problem which I once ran into when installing WSL 2, is virtualization not being enabled in my PC's BIOS. I was halted with the error:\n> \"Please enable the Virtual Machine Platform Windows feature and ensure virtualization is enabled in the BIOS.\"\n\nThe setting you need to enable in your BIOS may be called \"Intel Virtualization Technology\" or similar depending on your CPU and motherboard.\n\n## Synchronizing Clipboards\nWhile you may be able to copy text from your WSL terminal to your Windows clipboard by highlighting it with the mouse, programs in WSL such as Tmux or Vim copy text to the Linux clipboard, rather than to the Windows clipboard. In order to synchronize the two, the solution I've used is an X server, VcXsrv. To set this up you can follow these [instructions from a GitHub comment](https://github.com/Microsoft/WSL/issues/892#issuecomment-275873108), with one exception if you're on WSL 2. Due to the way networking was changed between WSL 1 & 2, instead of adding `export DISPLAY=localhost:0.0` to your `.bashrc` / `.zshrc` in step 5, add these two lines:\n```bash\nexport DISPLAY=$(awk '/nameserver / {print $2; exit}' /etc/resolv.conf 2>/dev/null):0\nexport LIBGL_ALWAYS_INDIRECT=1\n```\nAlso, those instructions don't mention how to have VcXsrv start up automatically. For that, you'll want to move the `config.xlaunch` file it creates to the Windows startup directory: `C:\\Users\\<YOUR-USER>\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\`\n\n## Accessing Files from Both Windows and WSL\nThis is something that has changed a lot between WSL 1 & 2. In WSL 1, opening files from the Linux filesystem with a Windows program [was a no go](https://devblogs.microsoft.com/commandline/do-not-change-linux-files-using-windows-apps-and-tools), and it was instead recommended to keep those shared files in the Windows filesystem, that both Windows and Linux could access. When I was using WSL 1, I would keep all my git repos in a Windows directory, and symlink it for quick access to my Ubuntu home directory, at `~/_`. I used this underscore directory as a sort of \"shared home directory\" for Windows & Linux.\n\nHowever in WSL 2, it is now recommended to keep those files in the Linux filesystem, for performance reasons. Now Windows is able to access the Linux filesystem as a network drive. You can view your Linux filesystem from the Windows File Explorer, by typing `\\\\wsl$` into its address bar. There's only one limitation I'm aware of with this network drive approach: symlinks.\n\nTaking my use case as an example, I keep my configuration files in a \"dotfiles\" git repo. When developing on WSL and using VSCode on Windows, I would need to keep my dotfiles repo in the Windows filesystem in order to create a Windows symlink (`mklink /D \"<target-dir>\" \"<source-dir>\"` in `cmd.exe`) that links those config files to the directory that VSCode looks for them. That's one case to consider before you put everything in the Linux filesystem.\n\n## Choosing a Terminal for WSL\nThe default terminal you get upon opening the Ubuntu Windows app (or other distro) is okay, but may be lacking some features or config options you've come to expect. Unless you're using Tmux, you're probably missing multiple terminal tabs. You can get this feature from the [Windows Terminal](https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701), from the Microsoft Store. My personal choice is [wsl-terminal](https://github.com/mskyaxl/wsl-terminal).\n"},{"title":"Hot Reloading Blog Preview on Markdown File Edit","date":"2021-02-20T16:23:26.604Z","slug":"hot-reloading-blog-preview-on-markdown-file-edit","content":"![Side by side web browser and vim hot reloading](/assets/blog-hot-reload.gif)\n\nWhen building this feature for my blog, what I wanted is the snappiness of an in-browser blog post editor, where you have a split view showing the editor on one side, and the rendered post on the other, instantaneously updated as you type into the editor. I used to use an in-browser editor for this purpose, but I now wanted the ability to edit inside my editor of choice, Vim.\n\nYou may have noticed in the gif above that the page only updates once I save the file. If you want something that updates as you type, you could opt for an auto-save solution like a plugin specific to your editor.\n\nSince I'm using Next.js, which comes with its own preconfigured dev server, I needed to customize the Next.js dev server to add this functionality. This isn't actually mandatory, as you could run an Express server separate from your Webpack / Next.js / other dev server, to be responsible for the file watching and WebSocket server.\n\nFor Next.js, there's some good suggestions for how to achieve this in [Next.js GitHub issue](https://github.com/vercel/next.js/discussions/11419). One of the suggestions I tried, the package [next-remote-watch](https://github.com/hashicorp/next-remote-watch), ended up being too sluggish for my liking. This is because the mechanism used is triggering an actual Next.js hot reload, the same as what happens when editing a source file.\n\nI ended up creating my own solution, utilizing:\n* File watching (via [chokidar](https://www.npmjs.com/package/chokidar))\n* WebSockets ([ws](https://www.npmjs.com/package/ws) for the dev server, and the built in browser [WebSocket API](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) for the client)\n* [Next.js custom server](https://nextjs.org/docs/advanced-features/custom-server) (only for development, not production)\n\nThese are the source files relevant for this feature:\n\n* [dev-server.ts](https://github.com/zzzachzzz/zzzachzzz.github.io/blob/master/dev-server.ts) - Custom Next.js server, responsible for file watching and the WebSocket server.\n* [/blog/edit/[slug].tsx](https://github.com/zzzachzzz/zzzachzzz.github.io/blob/master/pages/blog/edit/%5Bslug%5D.tsx) - Wrapper component for the blog post page, which connects to the WebSocket server.\n* [/blog/[slug].tsx](https://github.com/zzzachzzz/zzzachzzz.github.io/blob/master/pages/blog/%5Bslug%5D.tsx) - The blog post page, that receives either static props from Next, or when in edit mode, dynamic props from the above wrapper component that manages the WebSocket connection.\n"},{"title":"Going Truly Serverless with Next.js Static Site Generation","date":"2021-02-20T14:35:05.888Z","slug":"going-truly-serverless-with-nextjs-static-site-generation","content":"\nWith the hype of the [Jamstack](https://jamstack.org), and the benefits it offers, I made the switch from MERN stack to JAM stack for my blog. The most appealing benefits to me in my use case were:\n1. Improved SEO, for my site that can be 100% statically generated.\n2. Simplified architecture. No more databases and servers, just files served from GitHub Pages.\n3. Using Git as my \"CMS\". Switching from storing blog posts in a database, to storing them in `.md` files, tracked by Git.\n\n## Choosing a React Static Site Generator\n\nComing from a MERN app, I needed a SSG solution for React. I considered choosing between three different options:\n* [Gatsby](https://github.com/gatsbyjs/gatsby)\n* [Next.js](https://github.com/vercel/next.js)\n* [React Static](https://github.com/react-static/react-static)\n\nThe two most popular React frameworks are Gatsby & Next.js. Gatsby is a very powerful tool that emphasizes a plugin ecosystem. Gatsby can do a lot. As I went through Gatsby's vast documentation, I was having difficulty figuring out how to do what I wanted to do, for my fairly simple use case. After this experience, I was drawn to the simplicity of React Static. Ultimately, I ended up choosing Next.js for a few different reasons, one being their documentation coupled with their collection of examples on GitHub. I used their [blog-starter template](https://github.com/vercel/next.js/tree/canary/examples/blog-starter) as a starting point. Another reason I chose Next is their great out-of-the-box TypeScript support.\n\nThe last minor thing that sealed the deal for my decision, was the React team announcing that they are collaborating with the Next.js team on React's new experimental feature, \"Server Components\". Relevant links:\n* [Introducing Zero-Bundle-Size React Server Components](https://reactjs.org/blog/2020/12/21/data-fetching-with-react-server-components.html)\n* [(Timestamp in the talk where Dan Abramov mentions the Next.js collab)](https://youtu.be/TQQPAU21ZUw?t=2570)\n\nWhile not a huge factor, it gave me the impression that the design philosophies of Next.js and the React core team align the most closely, and that Next.js would be receiving first-class support for this feature.\n\n## Migrating from MERN to JAM\nSource code can be found here:\n* Post-migration GitHub source: <https://github.com/zzzachzzz/zzzachzzz.github.io/tree/master>\n* Pre-migration GitHub source: <https://github.com/zzzachzzz/zzzachzzz.github.io/tree/2ab6f0b10606162a57b946461c4dae74e2a295d5>\n\nAside from the usual steps to migrate a common React app to a Next.js React app, there were a few other things I needed to handle in the migration. There would be changes to accommodate for the removal of a server; no Express & no MongoDB. There would also be some small changes to account for a React app utilizing SSG or SSR (server-side rendering), specifically the way CSS is loaded, depending on the tool you use for CSS.\n\n### MongoDB to .md Files\nFor moving the content from MongoDB to markdown files, I created a migration script: [backend/migrations/db-to-markdown-file.js](https://github.com/zzzachzzz/zzzachzzz.github.io/blob/fc62221055adea46ef43803c973b28445262c448/backend/migrations/db-to-markdown-file.js)\n\n### CSS for SSG / SSR\nDepending on which tool you use for CSS, Next.js documents how to use it in SSG & SSR here: <https://nextjs.org/docs/basic-features/built-in-css-support>\n\nFrom there, they link to examples on their GitHub. I use `styled-components`, so I followed their example here: <https://github.com/vercel/next.js/tree/canary/examples/with-styled-components>\n\n### Hosting a Static Site on GitHub Pages\nIn addition to the `next build` script, to deploy a fully static site without a server, the `next export` script is used: <https://nextjs.org/docs/advanced-features/static-html-export>\n\nWhen hosting the static build on a git branch on GitHub Pages, the naming of the exported `_next/` directory triggers Jekyll on GitHub Pages, as directories with a leading underscore have a special meaning to Jekyll. This unexpected side effect resulted in 404s for certain static assets my site was trying to fetch. To disable Jekyll processing on GitHub pages, we need to provide a `.nojekyll` file at the root. This was my final build script in my `package.json` file:  \n`\"build\": \"next build && next export && touch ./out/.nojekyll\"`  \nMore info here: <https://github.blog/2009-12-29-bypassing-jekyll-on-github-pages>\n\nWhile not mandatory, I do recommend the [gh-pages](https://www.npmjs.com/package/gh-pages) npm package if you do intend to deploy to GitHub Pages. By default it will push to a branch named `gh-pages`. You'll want to configure your GitHub repo's settings to serve GitHub Pages from this branch. In conjunction with the build script I mentioned above, this was my deploy script:  \n`\"deploy\": \"yarn run build && gh-pages --dist out --dotfiles\"`  \n`out/` is the default output directory name for a Next.js static export, and we need to include the option `--dotfiles` for the `.nojekyll` file to be included in the push to the `gh-pages` branch.\n\n### Getting Prism.js to Work with SSG / SSR\nI never found it straightforward to set up Prism.js for code highlighting in a React app. When my pages were still being rendered client-side, I used `React.useEffect` to trigger Prism to highlight all code blocks:\n```jsx\nReact.useEffect(() => {\n  Prism.highlightAll();\n}, []);\n```\nWhile I could still have the effect be performed client-side, I wanted to go all in on having my site be fully statically generated. In my [TreeToJSX.tsx](https://github.com/zzzachzzz/zzzachzzz.github.io/blob/master/components/TreeToJSX.tsx) component, responsible for rendering a markdown document tree to JSX, I came up with the following solution to have the Prism highlighted HTML be built:\n```tsx\n// Add leading whitespace to <code> className due to className mismatch caused by Prism injecting class\nconst CodeBlock = ({ lang, children }: { lang?: string; children: string; }) => {\n  const langCls = ` language-${lang || 'none'}`;\n  if (lang) {\n    const highlightedCode = Prism.highlight(children, Prism.languages[lang], lang);\n    return (\n      <Pre className={langCls}>\n        <code className={\" \" + langCls} dangerouslySetInnerHTML={{__html: highlightedCode}}></code>\n      </Pre>\n    );\n  } else {\n    return (\n      <Pre className={langCls}>\n        <code className={\" \" + langCls}>{children}</code>\n      </Pre>\n    );\n  }\n};\n```\nPrism offers a low level `highlight` function that will return stringified HTML of the syntax highlighted code we provide in the string `children`. I was getting a warning about a mismatching className between the client and server (server in development, for SSG). The mismatch was caused by leading whitespace in one but not the other: `\" language-jsx\"` vs `\"language-jsx\"`. This was some oddity caused by the way Prism injects CSS classes, that I was able to workaround by adding leading whitespace to the class names.\n\n## Hot Reloading a Rendered Blog Post Upon its Markdown File Being Edited\n\nI outline this feature in a separate post:  \n[Hot Reloading Blog Preview on Markdown File Edit](/blog/hot-reloading-blog-preview-on-markdown-file-edit)\n\n![Side by side web browser and vim hot reloading](/assets/blog-hot-reload.gif)\n\n"},{"title":"Dockerizing a MERN App for Development and Production","date":"2020-10-25T16:30:43.234Z","slug":"dockerizing-a-mern-app-for-development-and-production","content":"\nCreating a Dockerfile for a single service usually isn't too bad. The example Dockerfile provided by the official guide for Node.js, [Dockerizing a Node.js web app](https://nodejs.org/en/docs/guides/nodejs-docker-webapp/), can be copied almost exactly.\n\nHowever, things start to get a little more complicated when we want to:\n* Create configurations for both development and production environments\n* Enable hot reloading in development (avoid needing Docker to re-build for every change)\n* Orchestrate connecting multiple services together (relevant for any web app with a frontend, backend, database, etc.)\n* Persist data in a database between runs (with Docker volumes)\n\nThe app I'll be using as an example can be found here: <https://github.com/zzzachzzz/zzzachzzz.github.io/tree/2ab6f0b10606162a57b946461c4dae74e2a295d5>  \nI will also include the various Docker files in this post.\n\n>**Edit (Feb. 15, 2021)**  \n>Yep, that's the source code for this site, at a prior commit. The site has since been migrated to Next.js with static site generation. To learn more about that, see the post:  \n[Going Truly Serverless with Next.js Static Site Generation](/blog/going-truly-serverless-with-nextjs-static-site-generation)\n\nTo Dockerize a React app, we'll definitely want a config for development, and production. In development, webpack-dev-server (`npm run [start|react-scripts-start]` in CRA) will be used with hot-reloading. In production, there are multiple ways to go about it, but I'll be using Nginx to serve the bundle, and proxying `/api` requests to the Express app.\n\n`frontend/Dockerfile.dev`:\n```dockerfile\nFROM node:14\nWORKDIR /usr/src/frontend\nCOPY package*.json ./\nRUN npm install\nEXPOSE 3000\nCMD [\"npm\", \"run\", \"start\"]\n```\n\nOne thing to note for proxying requests in development. If using CRA, you've likely set `\"proxy\": \"http://localhost:<port>\"` in `package.json` before, to proxy requests from React to a server, like Express. When running the frontend and the backend in separate Docker containers, they don't share the same localhost. Instead, we need to provide the network address created by Docker to connect the two together. You'll see more of this in later steps involving the Docker Compose `.yml` files, but as far as Webpack is concerned, we'll need to provide it a config file for the proxy: \n\n`frontend/src/setupProxy.js`:\n```js\nconst { createProxyMiddleware } = require('http-proxy-middleware');\n\nconst EXPRESS_HOST = process.env.EXPRESS_HOST || 'localhost';\n\nmodule.exports = function(app) {\n  app.use(\n    '/api',\n    createProxyMiddleware({\n      target: `http://${EXPRESS_HOST}:5000`\n    })\n  );\n};\n```\n\nSince I don't know of a way to embed an environment variable in the `package.json` file, this more advanced `setupProxy.js` file is necessary. Notice the environment variable `EXPRESS_HOST`. We will provide this variable to our Docker container, through our Docker Compose config. More on the above proxy config here: <https://create-react-app.dev/docs/proxying-api-requests-in-development/#configuring-the-proxy-manually>\n\n`frontend/Dockerfile.prod`:\n```dockerfile\nFROM node:14 as builder\nWORKDIR /usr/src/frontend\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nRUN npm run build\n\nFROM nginx\nCOPY --from=builder /usr/src/frontend/build /usr/share/nginx/html\nCOPY nginx.conf /etc/nginx/conf.d/\nEXPOSE 8080\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n```\n\nThis is considered a multi-stage Docker build, due to the multiple `FROM` statements. We build our Webpack bundle, beginning from the `node:14` base image, and then switch to the `nginx` base image to serve that Webpack bundle. Notice the line `COPY nginx.conf /etc/nginx/conf.d/`, which refers to a `nginx.conf` file I keep in Git.\n\n`frontend/nginx.conf`:\n```\nserver {\n    listen 8080;\n\n    location /api {\n        proxy_pass http://backend:5000;\n    }\n\n    location / {\n        root /usr/share/nginx/html;\n        try_files $uri /index.html;\n    }\n}\n```\n\n**Note that my `nginx.conf` is a bit abnormal**, since my server hosting the site has another Nginx instance running outside of Docker, which I have setup with `location / { proxy_pass http://localhost:8080; }`. I have it setup this way so I can host multiple sites, and have Nginx handle routing traffic based on the `server_name`. You'll probably want your `nginx.conf` setup to include sections for Certbot, to manage SSL certificates, and listen on port 80 & 443. Consult another tutorial on Certbot & Nginx for that.\n\nThe portion of this `nginx.conf` file that is applicable to you is the `proxy_pass` setup for `/api` requests, which sends it to the network host `backend` on port `5000`. This is Docker managing networking again. In this case, `backend`, is the name of our docker-compose service for Express, so Docker provides us the address for that specific container under the hostname `backend`.\n\nBefore we get to those Docker Compose files that link everything together, there's one more Dockerfile:\n\n`backend/Dockerfile`:\n```dockerfile\nFROM node:14\nWORKDIR /usr/src/backend\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nEXPOSE 5000\nCMD [\"node\", \"app.js\"]\n```\n\nFor the backend, I don't currently have a separate dev & prod Dockerfile, however I would recommend it, with the use of `nodemon` in place of `node` in the `CMD` statement in `Dockerfile.dev`, to enable hot reloading in development.\n\nNow onto the `docker-compose.yml` files. I have 3 of these under the filenames `docker-compose.yml`, `docker-compose.override.yml`, and `docker-compose.prod.yml`. You can choose different filenames, but there is a rational for these specific filenames. Both `docker-compose.yml` and `docker-compose.override.yml` are [filenames that Docker specifically looks for](https://docs.docker.com/compose/extends/). In both dev & prod, we use 2 of these 3 docker-compose files.\n\n* `docker-compose.yml` - The base config for dev & prod\n* `docker-compose.override.yml` - The dev config overrides\n* `docker-compose.prod.yml` - The prod config overrides\n\nAs shown in the Docker Compose docs linked above, multiple compose files can be specified with `-f` like so (also see `docker-compose --help`):  \n`docker-compose -f docker-compose.yml -f docker-compose.prod.yml [COMMAND] [ARGS...]`\n\nCompose files specified are read from left to right, which means `docker-compose.prod.yml` will be read last, giving it priority over our base `docker-compose.yml`.\n\nIf no files are specified with `-f`, Docker will do this:\n`docker-compose -f docker-compose.yml -f docker-compose.override.yml [COMMAND] [ARGS...]`\n\nWhy does this matter? In development, when you want to use the dev config, **you don't have to specify compose files for every `docker-compose` command you want to execute**. Running your entire application's stack in development becomes one short command: `docker-compose up`. That's it. Finally, here are those compose files:\n\n`docker-compose.yml`:\n```yml\nversion: \"3.8\"\n\nservices:\n  frontend:\n    build:\n      context: ./frontend\n    environment:\n      EXPRESS_HOST: backend\n\n  backend:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile\n    ports:\n      - \"5000:5000\"\n    env_file: ./backend/.env\n    environment:\n      HOST: 0.0.0.0\n      MONGO_HOST: mongo\n\n  mongo:\n    image: mongo:latest\n    ports:\n      - \"127.0.0.1:27017:27017\"\n    volumes:\n      - mongo-data:/data/db\n\nvolumes:\n  mongo-data:\n```\n\n`docker-compose.override.yml`:\n```yml\nservices:\n  frontend:\n    build:\n      dockerfile: ./Dockerfile.dev\n    ports:\n      - \"3000:3000\"\n    environment:\n      NODE_ENV: development\n    volumes:\n      - ./frontend:/usr/src/frontend\n      - /usr/src/frontend/node_modules\n    # Due to stupid react-scripts bug still present in v3.4.3\n    # https://github.com/facebook/create-react-app/issues/8688\n    stdin_open: true\n\n  backend:\n    environment:\n      NODE_ENV: development\n    volumes:\n      - ./backend:/usr/src/backend\n      - /usr/src/backend/node_modules\n```\n\n`docker-compose.prod.yml`:\n```yml\nservices:\n  frontend:\n    build:\n      dockerfile: ./Dockerfile.prod\n    ports:\n      - \"127.0.0.1:8080:8080\"\n    environment:\n      NODE_ENV: production\n\n  backend:\n    environment:\n      NODE_ENV: production\n```\n\nThere's a lot we could go over in these compose files. I mentioned in the beginning that we want to enable hot-reloading in development, so that changes to code made on our host computer, are reflected in our Docker container, triggering a hot-reload from webpack-dev-server (or nodemon). The way we reflect file changes between host and container is through specifying `volumes`:\n\n```yml\nvolumes:\n  - ./frontend:/usr/src/frontend\n  - /usr/src/frontend/node_modules\n```\n\n`./frontend:/usr/src/frontend` maps our host's `./frontend` directory, to our container's `/usr/src/frontend` directory. Since our host may have its own `node_modules` directory inside `./frontend`, we don't want this to be shared with the container. The container needs to maintain its own installed dependencies in isolation. To prevent our container's `node_modules` from being overwritten, we create an anonymous volume of our container's `/usr/src/frontend/node_modules` directory. The ordering of the two volumes listed is important, so that our container's `node_modules` stored in a volume have highest priority (applied last). I would recommend doing other research on Docker volumes to better understand the different types of volumes that Docker supports. Just note that we can tell Docker to persist its own `node_modules` by creating an anonymous volume (we don't assign a name to it), that Docker keeps track of with a generated hash as the volume name. This volume persists between container instances.\n\nOn the subject of volumes, another volume is very important to persist data in our Mongo database. Without a volume, our Mongo data would be lost every time out container stops and starts again, and we definitely don't want to lose our DB data, at least in production. You'll notice this volume is mentioned in two places:\n\n```yml\n  mongo:\n    image: mongo:latest\n    ports:\n      - \"127.0.0.1:27017:27017\"\n    volumes:\n      - mongo-data:/data/db\n\nvolumes:\n  mongo-data:\n```\n\nWhy does `mongo-data` appear twice? In this situation, we're using a \"named volume\" (again, highly recommend reading more on these volume types). A named volume behaves quite similar to the anonymous volume I described earlier, except it's named! We could technically make this volume anonymous, but it's good to be able to identify the volume in case we need to manipulate it somehow, like making a backup of our database data. Named volumes must be defined in the top-level `volumes` key, that's why it appears in two places, unlike anonymous volumes.\n\nSee the Docker Compose docs on volumes: <https://docs.docker.com/compose/compose-file/#volumes>\n\nYou may have noticed that ports are usually mapped like `\"<port>:<port>\"`, without specifying a host. With this shorthand, the host is implied to be `0.0.0.0`, listening on all interfaces. This makes it publicly accessible outside of the machine. If you don't need to directly access Mongo (via the Mongo shell) remotely, and can instead do so over SSH, I highly recommend that for security. Especially with the default Mongo config for Docker, there will be no credentials required to access Mongo. This means that an attacker who only knows the IP address of your server can remotely access your database! Yes, I did learn that the hard way, thankfully with non-consequential data. :sweat_smile: So do yourself a favor and specify the mapping `127.0.0.1:27017:27017` (`localhost` for the host, implicit `0.0.0.0` for the container).\n"},{"title":"How to Install Vim with +clipboard with Homebrew on Linux","date":"2020-02-08T17:08:57.528Z","slug":"how-to-install-vim-with-clipboard-with-homebrew-on-linux","content":"\n>Note: Or just install NeoVim and this should be a non-issue.\n\nInstalling Vim with brew on OSX has worked flawlessly for me, and included +clipboard support. In my experience, working with Windows Subsystem for Linux specifically, a simple `brew install vim` didn't cut it, and `vim --version` displayed that sad `-clipboard`. I would prefer to use the same package manager between OSX and Linux, especially since I use a shell script for installing all my brew packages. In the past I've just resorted to installing vim-gtk to get a clipboard enabled build of vim on Linux. However, vim-gtk only yielded me version 8.0, while brew offered 8.2. I cared enough about this to open a GitHub issue and get a solution.\n\n[Installing custom formula for Vim, options not present (+clipboard) - GitHub Issue](https://github.com/Homebrew/linuxbrew-core/issues/19505)\n\n1.  Install dependencies  \n    `sudo apt-get install libncurses5-dev libgnome2-dev libgnomeui-dev libgtk2.0-dev libatk1.0-dev libbonoboui2-dev libcairo2-dev libx11-dev libxpm-dev libxt-dev`\n2.  Modify the vim formula  \n    `brew edit vim`  \n    Change the configure option `--without-x` to `--with-x` and add the option `--with-features=huge`. Save the changes.\n3.  ```bash\n    system \"./configure\", \"--prefix=#{HOMEBREW_PREFIX}\",\n                          \"--mandir=#{man}\",\n                          \"--enable-multibyte\",\n                          # New options\n                          \"--with-x\",\n                          \"--with-features=huge\",\n    ```\n4.  Install the modified formula  \n    `brew install --build-from-source vim`\n\nIt is crucial to have the necessary dependencies installed. I tried these steps with the same formula options, `--with-x` and `--with-features=huge`, and my Vim installation _silently failed to include clipboard support, prior to installing the dependencies_. This is a major nuisance, and I hope to have raised some awareness of this issue, for a use case as common as installing Vim with clipboard support with Homebrew on Linux.\n"},{"title":"|, >, >>, <, <<, <<<, <() Piping and Redirection in the Shell","date":"2020-01-06T22:53:59.386Z","slug":"piping-and-redirection-in-the-shell","content":"\nLately I've been learning Vim more in depth, beyond just Vim's modal editing. With that, I've been learning more about Unix and the shell. As they say, \"Unix is an IDE\", and Vim is just one of its tools. I'm going to keep it simple and use the terms input & output to refer to stdin & stdout, the more technically correct terms here.\n\n`program > file` Redirects the output of a program to a file. If the file exists, it will be overwritten (be careful).\n\n`program >> file` Redirects the output of a program to a file. If the file exists, it will be appended to (safer option).\n\n`program < file` Redirects a file to be the input of a program. From what I can tell, this is rarely useful on its own, since nearly all programs which accept an input stream, also accept a file argument. Hence, these two are equivalent: `cat < file` & `cat file`. More details on that here:  \n[How does input redirection work? - Ask Ubuntu](https://askubuntu.com/a/883822)\n\n`output | program` Redirects the output of a program, to be the input of another program.  \nExample: `echo $PATH | less`  \nThis is functionally equivalent to:  \n`echo $PATH > temp_file && less < temp_file`\n\n## Herestrings & Heredocs\n\n[Here Strings - The Linux Documentation Project](https://tldp.org/LDP/abs/html/x17837.html)\n\n`program <<< string` Redirects a string to be the input of a program (stdin).  \nExample:  \n`python <<< \"print(len('Dude no way'))\"\n11`\n\n`program << delimiter\nmulti-line string\ndelimiter` Redirects a multi-line string to be the input of a program (stdin).  \nExample:  \n```bash\npython << EOF\nheredoc> print('Sooo')\nheredoc> print('Powerful')\nheredoc> EOF\nSooo\nPowerful\n```\nEOF (end of file) is just a convention here, the delimiter could be almost any sequence of characters.\n\n## Process Substitution\n\n`program <(output)` Redirects the output of a program to a temp file, to be treated as a file argument.  \nExamples:  \n```bash\n$ ls -l <(echo hi)\nlr-x------ 1 zach zach 64 Oct  3 07:35 /proc/self/fd/11 -> 'pipe:[23068]'\n\n$ cat <(echo hi)\nhi\n\n$ echo foo | python3 <(echo \"import sys; print('python stdin:', sys.stdin.read())\")\npython stdin: foo\n\n# Useful for vim diffing outputs, since vimdiff only accepts file arguments\n$ vimdiff <(/usr/local/bin/vim --version) <(/usr/bin/vim --version)\n...\n```\n"},{"title":"Common Tasks: JavaScript and Python Equivalents","date":"2019-11-13T05:06:20.818Z","slug":"common-tasks-javascript-and-python-equivalents","content":"\n## Template Strings\n\n```javascript\n// JavaScript\nlet name = 'Timmy';\nconsole.log(`${name}: ${3+9} btw haHAA`);  // Timmy: 12 btw haHAA\n```\n\n```python\n# Python (3.6+)\nname = 'Timmy';\nprint(f\"{name}: {3+9} btw haHAA\");  # Timmy: 12 btw haHAA\n```\n\n## Ternary Operator\n\n```javascript\n// JavaScript\nlet x = 0;\nx += true ? 1 : 0;\nconsole.log(x);  // 1\n```\n\n```python\n# Python\nx = 0\nx += 1 if True else 0\nprint(x)  # 1\n```\n\n## Array / List Manipulation\n\n```javascript\n// JavaScript\nlet x = [3];  // x: [3]\nx.push(5);  // x: [3, 5]\nlet y = x;  // x: [3, 5], y: [3, 5]\nconsole.log(x == y);  // true\n// Clone an array (shallow copy)\ny = [...x];  // x: [3, 5], y: [3, 5]\n// Or\ny = x.slice()\nconsole.log(x == y);  // false\n// Check for equality\nconsole.log(x.length === y.length && x.every((e, i) => e === y[i]))  // true\nx.pop()  // x: [3], y: [3, 5]\nconsole.log(x.length === y.length && x.every((e, i) => e === y[i]))  // false\n```\n\n```python\n# Python\nx = [3]  # x: [3]\nx.append(5)  # x: [3, 5]\ny = x  # x: [3, 5], y: [3, 5]\nprint(x is y)  # True\n# Clone a list (shallow copy)\ny = x.copy()  # x: [3, 5], y: [3, 5]\n# Or\ny = list(x)\nprint(x is y)  # False\n# Check for equality\nprint(x == y)  # True\nx.pop()  # x: [3], y: [3, 5]\nprint(x == y)  # False\n```\n\n## Reading & Writing JSON Files\n\n```javascript\n// JavaScript\nconst fs = require('fs');\n\nfs.writeFileSync('data.json', JSON.stringify({a: 1, b: 2}, null, 4));\nlet data = JSON.parse(fs.readFileSync('data.json'));\nconsole.log(data);  // {a: 1, b: 2}\n```\n\n```python\n# Python\nimport json\n\nwith open('data.json', 'w') as file:\n    json.dump({'a': 1, 'b': 2}, file, indent=4)\nwith open('data.json', 'r') as file:\n    data = json.load(file)\nprint(data)  # {'a': 1, 'b': 2}\n```\n\n## For Loops and Iteration\n\n```javascript\n// JavaScript\nconst arr = ['a', 'b'];\n\nfor (let i = 0; i < arr.length; i++) {\n  console.log(i, arr[i]);  // 0 'a' , 1 'b'\n}\n\nfor (const x of arr) {\n  console.log(x);  // a , b\n}\n\nconst obj = {a: 1, b: 2};\n\nfor (const key in obj) {\n  console.log(key, obj[key]);  // a 1 , b 2\n}\n\nfor (const [key, value] of Object.entries(obj)) {\n  console.log(key, value);  // a 1 , b 2\n}\n```\n\n```python\n# Python\narr = ['a', 'b']\n\nfor i in range(len(arr)):\n    print(i, arr[i])  # 0 a , 1 b\n\nfor x in arr:\n    print(x)  # a , b\n\nobj = {'a': 1, 'b': 2}\n\nfor key in obj:\n    print(key, obj[key])  # a 1 , b 2\n\nfor key, value in obj.items():\n    print(key, value)  # a 1 , b 2\n```\n\n## List Comprehension / Array Map\n\n```javascript\n// JavaScript\nlet arr = Array.from({length: 4}, _ => null);\nconsole.log(arr);  // [null, null, null, null]\n\narr = [1, 2, 3, 4].map(x => x % 2 === 0 ? true : false);\nconsole.log(arr);  // [false, true, false, true]\n\narr = [1, 2, 3, 4].filter(x => x % 2 === 0).map(x => x + 100);\nconsole.log(arr);  // [102, 104]\n// OR in a single iteration:\narr = [1, 2, 3, 4].reduce((filtered, x) => {\n  if (x % 2 === 0) filtered.push(x + 100);\n  return filtered;\n}, []);\nconsole.log(arr);  // [102, 104]\n```\n\n```python\n# Python\narr = [None for i in range(4)]\nprint(arr)  # [None, None, None, None]\n\narr = [True if x % 2 == 0 else False for x in [1, 2, 3, 4]]\nprint(arr)  # [False, True, False, True]\n\narr = [x + 100 for x in [1, 2, 3, 4] if x % 2 == 0]\nprint(arr)  # [102, 104]\n```\n\nMore to come\n\n\\_\\_\\_\n"},{"title":"A Practical Guide to Learning Vim","date":"2019-09-17T18:49:28.000Z","slug":"a-practical-guide-to-learning-vim","content":"\n**Edit (Jan. 11, 2020):** Since the creation of this blog post, I've begun using standalone Vim. My opinion hasn't changed about the learning curve, and I don't think there's an overall advantage to using Vim over using an IDE/Editor with a Vim plugin. My incentive for learning Vim more in depth is because I enjoy the process of mastering the skill. The rest of this blog post will be left in its original state. Also, the [IdeaVim](https://github.com/JetBrains/ideavim) plugin for JetBrains IDEs is the best I have used, even better than Neovintageous.\n\nA more fitting title might be \"A Practical Guide to _Adopting_ Vim\". I'm not an advocate of Vim as an editor, I'm a fan of modal editing, of a mouse-free text editing experience. I think Vim as an editor can be great after extensively customizing it to your liking, but again, even the process of learning how to customize Vim adds even more to the learning curve. For this reason, I recommend you continue using your editor of choice... _with a Vim plugin to enable modal editing_.\n\nOnce someone has learned the basics of Vim's keybindings, the next step is incorporating this new skill into their daily work, to develop the skill further, and train their muscle memory. One may attempt to switch to using Vim in their daily work, and quickly find their inability to be productive. Picking up Vim as an editor involves both learning Vim, and giving up all the features and keybindings you're accustomed to in your last editor. Be it VSCode, Sublime, Atom, even picking up one of these editors and maximizing your productivity in it by learning its features and keyboard shortcuts is not a trivial task.\n\n\"But Vim emulators suck, they're not as good as real Vim. Just use Vim you filthy casual.\" I read too many comments of this nature in r/vim...  \nWhile some plugins emulating Vim are worse than others, this statement of inferiority should not be a barrier to entry. People should learn Vim even only at a basic level. I'm not an expert, I don't do fancy Vim trick shots in my daily editing. The most I've done is a macro, and the use case for this becomes very rare when I have multiple cursors in Sublime. I love too many of Sublime's features to give it up! That's why I feel I've struck an excellent balance of Vim features and Sublime features with my configuration. That's the beauty of this, _you can keep the config you have and incrementally adopt Vim functionality, versus diving in head first._\n\nI'm sure you can achieve a similar level of customization and features using a plugin for some other editor of your choice. My point of reference is the Neovintageous plugin for Sublime Text, so that's what I'll be covering in the remainder of this post.\n\nVintage, Six, Vintageous, Neovintageous, which Vim plugin do I choose? I started with the Vintage package, which comes bundled with Sublime, but the package is set to be ignored by default. Sublime's stock offering of Vim emulation with Vintage met my needs just fine for a while, and I didn't see any reason to switch. The small tweaks I needed, such as binding jj to &lt;Esc>, was covered by a tweak to Sublime's JSON formatted settings.\n\nI eventually discovered the power of a well configured vimrc, and went in search for how I could include one in my Sublime Vim setup. Through some random comment I stumbled across on some forum, someone mentioned the capability of the Neovintageous package to allow you to include a .neovintageous file, the equivalent of a .vimrc file. I cannot believe that this feature is not advertised more on [Neovintageous's GitHub](https://github.com/NeoVintageous/NeoVintageous) readme. While they state that it is highly configurable, there is no mention of a .neovintageousrc file in the readme. I am shocked by this.\n\nThis is a killer feature, and for me it would have been the one thing to tempt me to abandon Sublime and force myself to learn all of Vim, beyond just its modal editing. For the record, I'm not allergic to Vim. I use it every day for quick file edits, but it doesn't have the allure that Sublime does on me. When you discover the power of a .vimrc, you may become addicted to making it your own, and improving upon some of Vim's less desirable default key mappings.\n\nI don't have a ton of modifications, but I'll share what I do have, as well as my motive behind each remap. If you want more, you can look on GitHub at some of the Vim customization repos with thousands of stars, and hundreds of lines of modifications. I prefer to keep it a little bit simpler. By the way, the file goes in your Sublime User folder with your other customizations. Example path on a Windows machine: `C:\\Users\\<your user>\\AppData\\Roaming\\Sublime Text 3\\Packages\\User\\.neovintageousrc`\n\nJust before I get into the `.neovintageousrc`, I know of one keybinding that needs to be assigned in the regular Sublime keybinding settings, and it's the most important one:\n\n```javascript\n{\n    \"keys\": [\"j\", \"j\"],\n    \"command\": \"_enter_normal_mode\",\n    \"args\": {\"mode\": \"mode_insert\"},\n    \"context\": [{\"key\": \"vi_insert_mode_aware\"}]\n},\n```\n\n`jj` for exiting insert mode, instead of having to reach over to press `<Esc>`. This is a popular one. `jk` is another good alternative. Onto the rest of the key mappings...  \n\n\n## `.neovintageousrc`\n\n`noremap J 5j` Shift + j moves the cursor down 5 lines  \n`noremap K 5k` Shift + k moves the cursor up 5 lines  \n`noremap W 5w` Shift + w moves the cursor forward 5 words  \n`noremap B 5b` Shift + b moves the cursor backwards 5 words\n\nThese 4 navigation mappings just make sense to me, `J` and `K` in particular. I don't think I'd seen these elsewhere. I don't like counting lines and words every time I wanna move around in Vim, so instead I would end up spamming `j` and `k`. I think this strikes a good balance for medium distance movement, without the need to first hit an arbitrary number key, and then my intended movement command. This way, I can just hold shift and fly around.  \n  \n  \n`noremap M J` Join lines with M\n\nAh but wait! I just replaced another very important default keybinding for joining lines with J. Yes, so I opted for a key next to it, M, which by default moves your cursor to the middle of the screen. This could also be a useful one, but I didn't find myself using it. Customize as you wish.  \n  \n  \n`noremap 0 ^`  \n`noremap ^ 0`\n\nHere I'm swapping the keys responsible for moving the cursor to the beginning of the line, and moving the cursor to the first non-whitespace character (skip the indentation). The latter I find far more useful, so I made it the key that's easier to press, `0`.  \n  \n  \n`noremap $ g_`\n\n`$` moves the cursor to the end of the line and includes the newline character. So if you yank and paste that, you'll get an extra line. `g_` is one of those random vim bindings under the g 'namespace', and it moves the cursor to the end of the line, excluding the newline character. Much better.  \n  \n  \n``noremap m `\nnoremap ` m``\n\nHere, I'm swapping the key for creating a mark and going to a mark. I go to marks far more often than I create them, so it makes sense to me to make goto mark easier to press with `m`.  \n  \n  \n`noremap <lt> ,` Translates to `noremap < ,` but `<` is a special character in the vimrc syntax  \n`noremap , ;`\n\nNormally when using `f` or `t` to find a character in a line, you would use `;` to jump to the next result, and `,` to jump to the previous result. What I've done here, is sort of mimic the logic of `n` and `N` for going to the next and previous result in a Vim search. You're using the same key, but holding `Shift` to do the reverse. This pattern is seen all throughout default Vim keybindings. With `,` and `<`, the mapping becomes much the same. You can think of it as `,` to jump to the next result, and `Shift + ,` to jump to the previous result.  \n  \n  \n`noremap <C-s> :w<CR>`\n\nMake the default keybind for saving in Vim also supported in Sublime.  \n  \n  \n`vnoremap <Tab> >gv\nvnoremap <S-Tab> <gv`\n\nI don't know all the technicalities of this one, but what it allows is for the repeated indentation and reverse indentation of a selection in visual mode, using `Tab` and `Shift + Tab`.\n\nThat's all I've got! Have fun customizing to make it your own ~\n"},{"title":"Git Reference: Keep it Simple. Common Workflows","date":"2019-09-03T19:28:34.000Z","slug":"git-reference-keep-it-simple-common-workflows","content":"\nAnywhere you see `<remote>`, you should probably use `origin`. More on origin vs upstream below.\n\n## Reference\n\n`git status` : display current branch and information on file changes\n\n`git branch` : view all local branches\n\n`git checkout <branch>` : switch to the specified branch\n\n`git branch -d <branch>` : delete a branch (may require `-D` to force if the branch has commits unique to it)\n\n`git checkout -b <name>` : create a new branch based on the current branch, specifying a name\n\n`git fetch` : download changes from remote\n\n`git merge <remote>/<branch>` : merge another branch into current branch\n\n`git pull` : fetch and merge the remote of the current branch into the local current branch\n\n`git rebase <branch>` : like merge, but move the current branch's unique commits to be the newest commits in the commit history\n\n`git push <remote> HEAD` : push current local branch to remote branch with the same name. If remote branch does not yet exist, requires `-u` flag to create it. If pushing to an existing branch after a rebase, `-f` flag to force will be required (be careful), as the commit history has been overwritten.\n\n`git diff` : view line additions and deletions for tracked modified files not yet added to staging\n\n`git diff --cached` : view line additions and deletions for files added to staging\n\n`git log` : view commit history with commit hashes\n\n`git reset` : remove all files from staging (implicit --soft)\n\n`git checkout -- <file or directory>` : Undo changes made to the specified file(s) (tracked and unstaged files)\n\n`git reset --hard <commit-hash>` or `git reset --hard <branch>` : reset current branch to match a specific commit or latest commit of a specified branch.  \nExample:  \n`git reset --hard 151fb5a134912aacec63969f479dd59e5057ff22`  \nor `git reset --hard master`.\n\n`git reflog` : view history of executed git commands. Undo Git actions with `git reset --hard <history-hash>` where history-hash is the hash shown in reflog of the state you want to go back to.\n\n## Resolve Merge Conflicts\n\n1.  `git status` to see files which contain conflicts\n2.  Open conflicting files in your editor and make desired changes, remembering to delete the lines added by Git containing `>>>>>>>`, `=======`, `<<<<<<<`.\n3.  `git add .` from top level directory of your git repo to add all files\n4.  `git merge --continue` to continue or `git merge --abort` at any time to cancel the merge\n\n## Reset my current branch to be exactly like remote master\n\n1.  `git fetch` : download changes from remote\n2.  `git reset --hard <remote>/master` : reset current branch to be identical to remote master branch\n\n## Origin vs Upstream?\n\nUpstream is only applicable in the context of GitHub Forks, as far as I know. There's a good explanation here: [Stack Overflow](https://stackoverflow.com/questions/9257533/what-is-the-difference-between-origin-and-upstream-on-github). You may want a workflow where you develop from your fork, get updates from the original repository, and push changes to your fork (perhaps to then open pull requests for the original repo). If that's the case, you want certain commands to specify upstream, and others origin:\n\n`git pull upstream master` (on local master) to update your clean local copy of the original repo as changes get merged to master.\n\nThen `git rebase master` or `git merge master` (on your feature branch) to update your feature branches.\n\n`git push -u origin HEAD` to push your feature branch to your own forked repo.\n"},{"title":"Multiple Inheritance in Python: Method Resolution Order (MRO)","date":"2019-08-30T16:08:56.000Z","slug":"multiple-inheritance-in-python-method-resolution-order-mro","content":"\n```python\nclass A:\n    def __init__(self):\n        print('A')\n\nclass B(A):\n    def __init__(self):\n        super().__init__()\n        print('B')\n\nclass C(A):\n    def __init__(self):\n        super().__init__()\n        print('C')\n\nclass D(B, C):\n    def __init__(self):\n        super().__init__()\n        print('D')\n\nd = D()\n```\n\nWhen class `D` is instantiated, what do you think will be the order of the print statements?\n\nPython's way of determining the order in which multiple inheritance is resolved is called the Method Resolution Order (MRO). The answer to the question is:\n\n```python\nA\nC\nB\nD\n```\n\nLets see why.\n\n```python\nclass D(B, C):\n    def __init__(self):\n        super().__init__()\n        print('D')\n```\n\nWe begin in class D, which inherits from both B and C. Here, Python resolves from right to left. What this means is that the first class listed takes precedence. If both class B and C defined a method, class B's version would be the one inherited by class D. Resolving right to left, `B <- C`, B's definition of the method would overwrite C's.\n\nSince Python resolves the inheritance of class `C` first, we jump into `C.__init__`.\n\n```python\nclass C(A):\n    def __init__(self):\n        super().__init__()\n        print('C')\n```\n\nBefore even reaching our print statement, we get redirected to `A.__init__`. Once in class A, there's nothing left to inherit, so we reach our first print statement `print('A')`. After that, we go back up the chain that got us to A. Next we hit `print('C')`, then finally we return to class D.\n\n```python\nclass D(B, C):\n    def __init__(self):\n        super().__init__()\n        print('D')\n```\n\nWe're still not done with `super().__init()`. We've resolved class C, now we're onto B.\n\n```python\nclass B(A):\n    def __init__(self):\n        super().__init__()\n        print('B')\n```\n\nSince B inherits from class A, don't we go back there again? Python keeps track of the order of inheritance, and disregards duplicate entries. We have already visited class A during our inheritance journey, so Python will skip going there again altogether. This is where Method Resolution Order comes in. `__mro__` is an attribute that can be called on a class (not an instance). Lets see what it returns:\n\n```python\nprint(D.__mro__)\n# (<class '__main__.D'>, <class '__main__.B'>, <class '__main__.C'>, <class '__main__.A'>, <class 'object'>)\n```\n\nLook at that, the order of our print statements was `A, C, B, D`. Python begins with the class 'object', and builds on it with each level of inheritance. `__mro__` and the order of our print statements are the same but reversed.\n\nFinishing our example, we skip going to class A inherited by B, since we already visited A when it was inherited by C. We hit `print('B')`, then finally return to class D and hit `print('D')`.\n"},{"title":"Understanding Promises in JavaScript","date":"2019-08-29T20:35:44.000Z","slug":"understanding-promises-in-javascript","content":"\nPromises are really confusing. There's a lot of keywords associated with promises and async JS:\n\n-   Promise\n-   resolve\n-   reject\n-   then\n-   catch\n-   async\n-   await\n\nFrom MDN:\n\n```javascript\nnew Promise(executor);\n```\n\n`executor`\n\n> A function that is passed with the arguments resolve and reject. The executor function is executed immediately by the Promise implementation, passing resolve and reject functions...\n\nSo if we break this down further, the expected arguments look like this:\n\n```javascript\nnew Promise(function executor(resolve, reject));\n```\n\nWe define the `executor` function, which receives `resolve` and `reject` callback functions as arguments.\n\n```javascript\nnew Promise(function executor(resolve, reject) {\n  if (true) {  // Success\n    resolve('Resolved the promise');\n  } else {  // Error\n    reject('Rejected the promise');\n  }\n});\n```\n\nThe `executor` function doesn't have to be named, I'm just making my example explicit.\n\nOnce the promise has been resolved or rejected, that will trigger either a chained `then` or `catch` to be called.\n\n```javascript\nnew Promise(function(resolve, reject) {\n  if (true) {  // Success\n    resolve('Resolved');\n  } else {  // Error\n    reject('Rejected');\n  }\n})\n.then(function(result) {\n  console.log(result);  // 'Resolved'\n})\n.catch(function(error) {\n  console.log(error);  // 'Rejected'\n});\n```\n\nAn alternative syntax (that I think is more confusing), is to pass a second argument to `then`, which will handle `reject` in place of `catch`. I would not recommend this but it's good to know.\n\n```javascript\nnew Promise(function executor(resolve, reject) {\n  if (true) {  // Success\n    resolve('Resolved');\n  } else {  // Error\n    reject('Rejected');\n  }\n})\n.then(function(result) {\n  console.log(result);  // 'Resolved'\n}, function(error) {\n  console.log(error);  // 'Rejected'\n});\n```\n\nNow that we understand the general behavior of Promises, let's break down this example `sleep` function.\n\n```javascript\nfunction sleep(ms) {\n  return new Promise(function(resolve) {\n    setTimeout(function() {\n      resolve('Resolved Value');\n    }, ms);\n  })\n}\n\nasync function main() {\n  console.log('Before sleep');\n  const resolvedValue = await sleep(5000);  // 5 seconds\n  console.log('After sleep');\n  console.log(resolvedValue);  // 'Resolved Value'\n}\n\nmain();\n```\n\nNormally if we were to call `sleep` without `await`, there would be no 5 second pause between the two log statements. That's because our `sleep` function returns a promise, so we must wait for it to be resolved if we want our `main` function to be executed synchronously.\n\nIn order to use the `await` keyword, the surrounding function, `main`, must be given the `async` keyword.\n\n`setTimeout` is an asynchronous function, one of few in JS's built in library. However, `setTimeout`_ is not a Promise-based asynchronous function, it is callback-based_. In order to `await` for `setTimeout` to complete, we must wrap it in a promise, and `resolve` that promise inside the callback function we provide to `setTimeout`.\n\nHere's a shorthand version of the above using arrow functions.\n\n```javascript\nconst sleep = ms => new Promise(resolve => setTimeout(() => resolve('Resolved Value'), ms));\n\nasync function main() {\n  console.log('Before sleep');\n  const resolvedValue = await sleep(5000);  // 5 seconds\n  console.log('After sleep');\n  console.log(resolvedValue);  // 'Resolved Value'\n}\n\nmain();\n```\n\nPromises are confusing. If you don't understand them fully, don't worry about it. Check out other resources, and most importantly, play around with the code yourself.\n"},{"title":"Slate.js: Draft.js without the Bad Parts","date":"2019-08-28T15:37:27.000Z","slug":"slatejs-draftjs-without-the-bad-parts","content":"\nAnyone who has used Facebook's open source package, `Draft.js` knows that while it's a powerful tool for building rich text editors, the API docs are underdeveloped, and can be very difficult to understand. The editor I wrote this blog post in was made by me with `Slate.js`, and before I found that, I was struggling to learn how to make Draft.js do what I wanted it to do. I don't have the expertise to go too into detail about comparing Slate and Draft, but a lot of that is covered here in the readme of Slate: [Slate Principles](https://github.com/ianstormtaylor/slate#principles). Instead I'll tell you about my use case: what I wanted to build with Draft, the problems I ran into, and how Slate made the process easier for me.\n\nGiven that this is a programming blog, the most important feature to me is beautiful code snippets with syntax highlighting. Like so:\n\n```javascript\n// A JavaScript comment\nconst language = 'JavaScript';\nconsole.log(`This is definitely ${language}`);  // This is definitely JavaScript\n```\n\nI had `Prism.js` to handle the syntax highlighting.\n\nI needed my editor to...\n\n1.  Handle multi-line code blocks\n2.  Keep all those lines within the tags `<pre><code> ... </code></pre>` (To comply with how Prism.js works)\n3.  Have a way to specify the syntax (via adding a css class to a `<code>` tag: `language-javascript`)\n\nNot too many requirements. This seemed very doable in Draft, especially considering that the example on [draftjs.org](https://draftjs.org) features a Code Block styling option. If you click the code block toggle, make it a couple lines long, and inspect the page, you'll see that each line has it's own `<pre>` tag. Ah, that fails requirement #2 for me, but I'm sure I can customize mine to behave differently.\n\nAt the end of the day, I did just want a rich text editor, much like the one on Draft.js' home page, but with my customized code blocks. I found an npm package `react-rte`, which is pretty much a pre-built rich text editor built on Draft.js, with the ability to customize _certain_ things further (with this package, Draft was being abstracted away, and I could only customize parts that the creators of react-rte designated). Out of the box, react-rte did have multi-line code blocks contained in a single `<pre>` tag, and the ability to insert a soft line-break to stay inside a code block with Shift + Enter. Not bad, until you try to _**paste**_ a bunch of code in... which is the main way I would be inserting code into blog posts, pasting it from my real editor.\n\nSo how do you solve this problem?\n\nWhat would be needed is a special condition for code blocks. If the content block is a code block, insert another code line, without exiting that parent code block. _I'm sure this type of behavior would have been possible in Draft.js,_ but the trouble was figuring out _how_.\n\nThis is the main difference between Draft and Slate: Slate gives you tons of examples _with_ those examples being demonstrated on Slate's website, an excellent getting started walkthrough, and an API that actually makes sense.\n\nBecause of these offerings, my experience with Slate went something like this...\n\n1.  Read the Walkthrough (a very pleasant experience might I add)\n2.  Refer to examples... _\"Oh look, 'Rich Text' and 'Code Highlighting' are exactly what I want, I'll just look at the source code for these and combine them into my own!\"_\n3.  Complete customization of my editor, referring to Slate's API docs\n\nAnd there are many more examples I'll be pulling from. At the time of writing this post, my editor doesn't have support for images, but there's an example on Slate's site I'll be referring to.\n\nHappy custom editor building ~\n"}]},"__N_SSG":true}